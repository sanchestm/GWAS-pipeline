{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92d4405-4d52-4ac5-b675-131ef943f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsReport\n",
    "from sklearn.preprocessing import QuantileTransformer, MinMaxScaler\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.base import is_classifier, is_regressor\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "import math\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from datetime import datetime\n",
    "import time\n",
    "import umap\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from prophet import Prophet\n",
    "from umap import UMAP\n",
    "from lightgbm import LGBMRegressor,LGBMClassifier, plot_tree\n",
    "from sklearn.preprocessing import Binarizer,FunctionTransformer, KBinsDiscretizer, KernelCenterer, LabelBinarizer, LabelEncoder, MinMaxScaler,MaxAbsScaler,\\\n",
    "                                  QuantileTransformer, Normalizer, OneHotEncoder, OrdinalEncoder,PowerTransformer, RobustScaler, SplineTransformer,StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import DictionaryLearning,FastICA, IncrementalPCA, KernelPCA, MiniBatchDictionaryLearning, MiniBatchSparsePCA, NMF,PCA,SparsePCA, FactorAnalysis,\\\n",
    "                                  TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.manifold import LocallyLinearEmbedding, Isomap, MDS, SpectralEmbedding, TSNE\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import estimator_html_repr\n",
    "#import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import  BaseEnsemble,RandomForestClassifier, RandomForestRegressor, RandomTreesEmbedding, ExtraTreesClassifier, ExtraTreesRegressor,\\\n",
    "                          BaggingClassifier, BaggingRegressor, IsolationForest, GradientBoostingClassifier, GradientBoostingRegressor, AdaBoostClassifier,\\\n",
    "                          AdaBoostRegressor, VotingClassifier, VotingRegressor, StackingClassifier, StackingRegressor, HistGradientBoostingClassifier,\\\n",
    "                          HistGradientBoostingRegressor\n",
    "from sklearn.linear_model import BayesianRidge, ElasticNet, ElasticNetCV, Hinge, Huber, HuberRegressor, Lars, LarsCV, Lasso, LassoCV, LassoLars, LassoLarsCV, LassoLarsIC,\\\n",
    "                             LinearRegression, Log, LogisticRegression, LogisticRegressionCV, ModifiedHuber,MultiTaskElasticNet, MultiTaskElasticNetCV, MultiTaskLasso,\\\n",
    "                             MultiTaskLassoCV, OrthogonalMatchingPursuit, OrthogonalMatchingPursuitCV, PassiveAggressiveClassifier, PassiveAggressiveRegressor, Perceptron, \\\n",
    "                             QuantileRegressor, Ridge, RidgeCV, RidgeClassifier, RidgeClassifierCV, SGDClassifier, SGDRegressor, SGDOneClassSVM, SquaredLoss,TheilSenRegressor, \\\n",
    "                             RANSACRegressor, PoissonRegressor,GammaRegressor,TweedieRegressor\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB,ComplementNB, CategoricalNB\n",
    "from scipy.stats import ttest_ind, ttest_1samp\n",
    "from  plotly.offline  import plot_mpl\n",
    "import plotly.tools as ptools\n",
    "import networkx as nx\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "import calendar\n",
    "from prophet.utilities import regressor_coefficients \n",
    "import plotly.express as px\n",
    "import base64\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import io\n",
    "#from keplergl import KeplerGl\n",
    "import hdbscan\n",
    "import datetime\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "#from jupyter_dash.comms import _send_jupyter_config_comm_request\n",
    "#_send_jupyter_config_comm_request()\n",
    "from jupyter_dash import JupyterDash\n",
    "import dash_cytoscape as cyto\n",
    "import dash\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash import html, dcc, dash_table\n",
    "from dash.exceptions import PreventUpdate\n",
    "import dash_bootstrap_components as dbc\n",
    "from dash.dependencies import Input, Output# Load Data\n",
    "from dash.dash_table.Format import Format, Scheme, Trim\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from joblib import Memory\n",
    "from shutil import rmtree\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import auc,confusion_matrix,classification_report\n",
    "from sklearn.metrics import RocCurveDisplay,ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from skopt import BayesSearchCV, gp_minimize, forest_minimize, gbrt_minimize\n",
    "from skopt.searchcv import BayesSearchCV as BSCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold, KFold\n",
    "from skopt.plots import plot_objective\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.plots import plot_convergence\n",
    "from sklearn.feature_selection import RFECV\n",
    "set_config(display='diagram') \n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.base import clone\n",
    "import plotly.figure_factory as ff\n",
    "import statsReport\n",
    "\n",
    "import dash_bio as dashbio\n",
    "from glob import glob\n",
    "import gzip\n",
    "import urllib.request as urlreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e07fa3-e656-47fa-aaa2-0e8bafe4748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build App\n",
    "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.MINTY]) #FLATLY, LUMEN, SUPERHERO\n",
    "server = app.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb68632a-4584-47dc-b515-beb00c744988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source_data = urlreq.urlopen(\n",
    "#    'https://palmerlab.s3.sdsc.edu/minio/tsanches_dash_genotypes/Palmer_bi_SNPs_concor_hom.vcf'\n",
    "#).read().decode('utf-8')[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62335eab-37b1-4cc8-9c1c-5b67298ee59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "founders = {\n",
    "    'vcf' : 'https://palmerlab.s3.sdsc.edu/tsanches_dash_genotypes/Palmer_bi_SNPs_concor_hom.vcf.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=y7WRaXAs%2F20221129%2F%2Fs3%2Faws4_request&X-Amz-Date=20221129T221837Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=732ae054094f076c825dc46c511c93a40fc05dca5eb4c1490f57650f63fab0a9',\n",
    "    'index': 'https://palmerlab.s3.sdsc.edu/tsanches_dash_genotypes/Palmer_bi_SNPs_concor_hom.vcf.gz.tbi?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=y7WRaXAs%2F20221129%2F%2Fs3%2Faws4_request&X-Amz-Date=20221129T221920Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=5ffb01d00013bfd088577ca3a656faca0cc11748e5b55d1aa7213eb940af922e'\n",
    "}\n",
    "RN6_data = {\n",
    "\n",
    "'rat_genome_ref': {\n",
    "    'label': 'Rat rn6',\n",
    "    'url': 'https://hgdownload.soe.ucsc.edu/gbdb/rn6/rn6.2bit'},\n",
    "\n",
    "'tracks':[{ 'viz': 'scale', 'label': 'Scale' },  { 'viz': 'location', 'label': 'Location'},  \n",
    "    {'viz': 'genotypes','label': 'Founder genotypes', 'source': 'vcf', 'sourceOptions': {'url':founders['vcf']}},\n",
    "     {'viz': 'genes', 'label': 'Uniprot Full', 'source': 'bigBed', 'sourceOptions': {'url': 'https://hgdownload.soe.ucsc.edu/gbdb/rn6/ncbiRefSeq/ncbiRefSeqOther.bb'}},\n",
    "     #{'viz': 'features', 'label': 'clinvar', 'source': 'bigBed', 'sourceOptions': {'url': 'https://hgdownload.soe.ucsc.edu/gbdb/rn6/bbi/evaSnp.bb'}},\n",
    "    ]\n",
    "\n",
    "}\n",
    "\n",
    "gwas_link_list =[\n",
    " 'https://palmerlab.s3.sdsc.edu/tsanches_dash_genotypes/tomJhow_gwas/gwas/runstartmale1.loco.mlma?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=y7WRaXAs%2F20221116%2F%2Fs3%2Faws4_request&X-Amz-Date=20221116T224547Z&X-Amz-Expires=432000&X-Amz-SignedHeaders=host&X-Amz-Signature=c1e41b4e5ddb03261b27d38bb332a47b2cc186412dffb3e79ee91ef43aeab0a1',\n",
    " 'https://palmerlab.s3.sdsc.edu/tsanches_dash_genotypes/tomJhow_gwas/gwas/runstartfemale1.loco.mlma?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=y7WRaXAs%2F20221116%2F%2Fs3%2Faws4_request&X-Amz-Date=20221116T224851Z&X-Amz-Expires=432000&X-Amz-SignedHeaders=host&X-Amz-Signature=11df7746758efe1e10f409cc3d48a1dd9cd691daf1e2427d2c487c5e2f11d113',\n",
    "'https://palmerlab.s3.sdsc.edu/tsanches_dash_genotypes/tomJhow_gwas/gwas/runstart1.loco.mlma?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=y7WRaXAs%2F20221116%2F%2Fs3%2Faws4_request&X-Amz-Date=20221116T225532Z&X-Amz-Expires=432000&X-Amz-SignedHeaders=host&X-Amz-Signature=a93f5ce0b1cdc2827e75c7482c5b448b1c75cea7cc1c0894f04718e295c7c91a',\n",
    "]\n",
    "#'https://hgdownload.soe.ucsc.edu/gbdb/rn6/uniprot/unipFullSeq.bb'\n",
    "### vcf from the lab - visit https://palmerlab.s3.sdsc.edu/minio/tsanches_dash_genotypes/\n",
    "###https://palmerlab.s3.sdsc.edu/tsanches_dash_genotypes/Palmer_HS_founders_mRatBN7_2_ballele_subset_20221024_tosubset.vcf.gz?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=y7WRaXAs%2F20221116%2F%2Fs3%2Faws4_request&X-Amz-Date=20221116T191737Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=e0c5d2b7180c9f4d0f4d16cc2d4f7be543534d39a1826ac5f7a6b7386fe197a9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "859730cc-ebb7-43c5-9263-2592eaae8ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    }
   ],
   "source": [
    "cyto.load_extra_layouts()\n",
    "\n",
    "class pipemaker2:\n",
    "    def __init__(self, df,ipt_pipe, target ,*, height = 'auto', width = 'auto'):\n",
    "        self.pipe_list = []\n",
    "        self.df = df\n",
    "        self.TG = target\n",
    "        self.check = 0\n",
    "        self.cached_pipe = 0\n",
    "        self.location = 0\n",
    "        self.memory = 0\n",
    "        self.optimized_pipe = (0, 0)\n",
    "        self.input_pipe = ipt_pipe\n",
    "        \n",
    "    def Pipe(self):\n",
    "        return clone(self.input_pipe)\n",
    "    \n",
    "    def Cache_pipe(self):\n",
    "        self.location = 'cachedir'\n",
    "        self.memory = Memory(location=self.location, verbose=0)\n",
    "        self.cached_pipe = self.Pipe().set_params(memory = self.memory)\n",
    "    \n",
    "    def release_cache(self):\n",
    "        self.memory.clear(warn=True)\n",
    "        rmtree(self.location)\n",
    "        del self.memory\n",
    "        \n",
    "    def export_kwards(self):\n",
    "        return self.Pipe().get_params()\n",
    "    def fit_transform(self):\n",
    "        return self.ColumnTransform().fit_transform(self.df)\n",
    "    def fit_predict(self):\n",
    "        return self.Pipe().fit_predict(self.df, self.df[self.TG])\n",
    "    def fit(self):\n",
    "        return self.Pipe().fit(self.df, self.df[self.TG])\n",
    "    \n",
    "    def RFECV(self):\n",
    "        preprocessed_df = pd.DataFrame(self.Pipe()['preprocessing'].fit_transform(self.df))\n",
    "        \n",
    "        if self.optimized_pipe[1] == 0:\n",
    "            selector = RFECV(self.Pipe()['classifier'], step=1, cv=KFold(10, shuffle= True)).fit(preprocessed_df, self.df[self.TG])\n",
    "        else:\n",
    "            selector = RFECV(self.optimized_pipe[0]['classifier'], step=1, cv=KFold(10, shuffle= True)).fit(preprocessed_df, self.df[self.TG])\n",
    "            \n",
    "        hX = np.array( range(1, len(selector.grid_scores_) + 1))\n",
    "        hY= selector.grid_scores_\n",
    "        H = pd.DataFrame(np.array([hX, hY]).T, columns = ['Number of parameters', 'Cross Validation Score'])\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.xlabel(\"Number of features selected\")\n",
    "        plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "        plt.plot(hX, hY)\n",
    "        plt.show()\n",
    "        return pd.DataFrame([selector.ranking_, selector.support_], columns = preprocessed_df.columns, index = ['Ranking', 'support'])\n",
    "    \n",
    "    def make_skpot_var(self, param, temperature = 3, distribution = 'uniform', just_classifier = False): #'log-uniform'\n",
    "        value = self.export_kwards()[param]\n",
    "        if just_classifier == True: name = param.split('__')[1]\n",
    "        else: name = param\n",
    "        \n",
    "        if value == 0 or value ==1: return\n",
    "        \n",
    "        if type(value) == int: \n",
    "            if value == -1: return Integer(1, 200, name = name)\n",
    "            lower_bondary = int(value/temperature)\n",
    "            if lower_bondary < 2: lower_bondary = 2\n",
    "            upper_bondary = int(value*temperature) + lower_bondary\n",
    "            #if value <= 1: return Real(1e-3, 1, distribution ,name = name)\n",
    "            return Integer(lower_bondary, upper_bondary, distribution ,name = name)\n",
    "\n",
    "        if type(value) == float:\n",
    "            if value == -1: return Real(1, 200, name = name)\n",
    "            if value <= 1: return Real(1e-3, 1, distribution ,name = name)\n",
    "            lower_bondary = value/temperature\n",
    "            if lower_bondary < 2: lower_bondary = 2\n",
    "            upper_bondary = value*temperature + lower_bondary\n",
    "            return Real(lower_bondary, upper_bondary, distribution ,name = name)\n",
    "\n",
    "    def skopt_classifier_space(self, just_classifier = False):\n",
    "        dic = self.export_kwards()\n",
    "        classifier_params = [x for x in  dic.keys() \n",
    "                             if x.find('classifier__') != -1 \n",
    "                             and  x.find('silent') == -1 \n",
    "                             and  x.find('n_jobs') == -1\n",
    "                             and x.find('bagging_fraction') == -1 \n",
    "                             and x != 'classifier__subsample'\n",
    "                             and x != 'classifier__validation_fraction'] # and\n",
    "        SPACE = [self.make_skpot_var(i, just_classifier = just_classifier) for i in classifier_params]\n",
    "        SPACE = [x for x in SPACE if x if x != None ]\n",
    "        return SPACE\n",
    "\n",
    "    def objective(self, params):\n",
    "        classifier = self.Pipe().set_params(**{dim.name: val for dim, val in zip(self.skopt_classifier_space(), params)})\n",
    "        return -np.mean(cross_val_score(classifier, self.df, self.df[self.TG], cv = StratifiedKFold(n_splits = 5, shuffle=True)))\n",
    "    \n",
    "    def objective_just_classifier(self, params, metric , cv_method ):\n",
    "        return -np.mean(cross_val_score(self.cached_pipe['classifier'].set_params(**{dim.name: val for dim, val in zip(self.skopt_classifier_space(just_classifier = 1), params)}), \n",
    "                                        self.transformed_opt, \n",
    "                                        self.target_opt,\n",
    "                                        scoring = metric,\n",
    "                                        cv = cv_method, \n",
    "                                        n_jobs = -1))\n",
    "    \n",
    "    def objective_cached(self, params):\n",
    "        return -np.mean(cross_val_score(self.cached_pipe.set_params(**{dim.name: val for dim, val in zip(self.skopt_classifier_space(), params)}),\n",
    "                                        self.df, \n",
    "                                        self.df[self.TG], \n",
    "                                        cv = StratifiedKFold(n_splits = 5, shuffle=True)))\n",
    "    \n",
    "    \n",
    "    def optimize_classifier(self, n_calls = 50, cache = False):\n",
    "        if cache: \n",
    "            self.Cache_pipe()\n",
    "            result = gp_minimize(self.objective_cached, self.skopt_classifier_space() , n_calls=n_calls)\n",
    "            self.release_cache()\n",
    "        else: result = gp_minimize(self.objective, self.skopt_classifier_space() , n_calls=n_calls)\n",
    "        #plot_convergence(result)\n",
    "        #_ = plot_objective(result, n_points=n_calls)\n",
    "        #print(result.fun)\n",
    "        return {'result': result, 'best_params': self.get_params(result, self.skopt_classifier_space() )} \n",
    "    \n",
    "    def fast_optimize_classifier(self, n_calls = 50,  is_classifier = True):\n",
    "        self.Cache_pipe()\n",
    "        \n",
    "        self.transformed_opt = self.cached_pipe['preprocessing'].fit_transform(self.df)\n",
    "        self.target_opt = self.df[self.TG]\n",
    "        \n",
    "        if is_classifier: \n",
    "            cv_method = StratifiedKFold(n_splits = 5, shuffle=True)\n",
    "            metric    = 'f1_weighted'\n",
    "        else:      \n",
    "            cv_method = KFold(n_splits = 5, shuffle=True)\n",
    "            metric    = 'r2'\n",
    "        \n",
    "        result = gp_minimize(lambda x: self.objective_just_classifier(x, metric, cv_method), self.skopt_classifier_space(just_classifier = True) , n_calls=n_calls)\n",
    "        self.release_cache()\n",
    "        \n",
    "        best_params = self.get_params(result, self.skopt_classifier_space(just_classifier = True))\n",
    "        best_params = {'classifier__'+ i[0]:i[1] for i in best_params.items()}\n",
    "        \n",
    "        self.optimized_pipe = (self.Pipe().set_params(**best_params), 1)\n",
    "\n",
    "        return {'result': result, 'best_params':best_params} \n",
    "\n",
    "    def get_params(self, result_object, space):\n",
    "        try:\n",
    "            return { i.name: result_object.x[num] for  num, i in enumerate(space) }\n",
    "        except:\n",
    "            raise\n",
    "             \n",
    "    def Vis_Cluster(self, method):\n",
    "        transformed = self.Pipe()['preprocessing'].fit_transform(self.df)\n",
    "        classsification = method.fit_predict(transformed)  #(*args, **kwds)\n",
    "        end_time = time.time()\n",
    "        palette = sns.color_palette('deep', np.unique(classsification).max() + 1)\n",
    "        colors = [palette[x] if x >= 0 else (0.0, 0.0, 0.0) for x in classsification]\n",
    "        plt.scatter(transformed.T[0], transformed.T[1], c=colors, s = MinMaxScaler(feature_range=(30, 300)).fit_transform(self.df[self.TG].values.reshape(-1, 1)) , **{'alpha' : 0.5,  'linewidths':0})\n",
    "        frame = plt.gca() \n",
    "        for num, spine in enumerate(frame.spines.values()):\n",
    "            if num == 1 or num == 3: spine.set_visible(False)\n",
    "        plt.title('Clusters found by {}'.format(str(method)), fontsize=24)\n",
    "        plt.show()\n",
    "        return \n",
    "    \n",
    "    def Evaluate_model(self):\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        prd = []\n",
    "        tru = []\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        X = self.df.copy()\n",
    "        y = self.df[self.TG]\n",
    "        if self.optimized_pipe[1] == 0: clf = self.Pipe()\n",
    "        else: clf = self.optimized_pipe[0]\n",
    "        fig, ax = plt.subplots(1, 2, figsize = (20,10))\n",
    "        try:\n",
    "            for i, (train, test) in enumerate(StratifiedKFold(n_splits=5, shuffle=True).split(X, y)):\n",
    "                clf.fit(X.iloc[train], y.iloc[train])\n",
    "                viz = RocCurveDisplay.from_estimator(clf, X.iloc[test], y.iloc[test],\n",
    "                                     name='ROC fold {}'.format(i),\n",
    "                                     alpha=0.3, lw=1, ax=ax[0])\n",
    "                interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "                interp_tpr[0] = 0.0\n",
    "                tprs.append(interp_tpr)\n",
    "                aucs.append(viz.roc_auc)\n",
    "\n",
    "            ax[0].plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "                    label='Chance', alpha=.8)\n",
    "\n",
    "            mean_tpr = np.mean(tprs, axis=0)\n",
    "            mean_tpr[-1] = 1.0\n",
    "            mean_auc = auc(mean_fpr, mean_tpr)\n",
    "            std_auc = np.std(aucs)\n",
    "            ax[0].plot(mean_fpr, mean_tpr, color='b',\n",
    "                    label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "                    lw=2, alpha=.8)\n",
    "\n",
    "            std_tpr = np.std(tprs, axis=0)\n",
    "            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "            ax[0].fill_between(mean_fpr, tprs_lower, tprs_upper, color='steelblue', alpha=.2,\n",
    "                            label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "            ax[0].set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05])\n",
    "            #       title=\"Receiver operating characteristic example\")\n",
    "            ax[0].legend(loc=\"lower right\")\n",
    "        except: print('non-binary classifier')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "        try:\n",
    "            ConfusionMatrixDisplay.from_estimator(clf.fit(X_train, y_train), X_test, y_test,\n",
    "                                         display_labels=['negative detection', 'positive detection'],\n",
    "                                         cmap=plt.cm.Blues, ax = ax[1])\n",
    "            ax[1].grid(False)\n",
    "        except: print('is it a regressor?')\n",
    "        fig.tight_layout()\n",
    "        try: \n",
    "            report = classification_report(clf.predict(X_test), y_test, output_dict=True) # target_names=['Negative detection', 'Positive detection']\n",
    "        except: #### report for regression\n",
    "            if self.optimized_pipe[1] == 0: clf = self.Pipe()\n",
    "            else: clf = self.optimized_pipe[0]\n",
    "            report = cross_validate(clf, X, y, cv=5,  scoring=('neg_mean_absolute_percentage_error','r2','explained_variance', 'max_error', 'neg_mean_absolute_error', 'neg_mean_squared_error'))\n",
    "            fig, ax = plt.subplots(1, 1, figsize = (10,10))\n",
    "            fig.tight_layout()\n",
    "        return report, fig\n",
    "        \n",
    "    def named_preprocessor(self):  \n",
    "        naming_features = []\n",
    "        for transformer in self.Pipe()['preprocessing'].transformers:\n",
    "            transformed = ColumnTransformer(transformers = [transformer]).fit_transform(self.df)\n",
    "            if transformed.shape[1] == len(transformer[2]):\n",
    "                naming_features += list(transformer[2])\n",
    "            else:\n",
    "                naming_features += [transformer[0] +'__'+ str(i) for i in range(transformed.shape[1]) ]\n",
    "        if self.optimized_pipe[1] == 0: clf = self.Pipe()\n",
    "        else: clf = self.optimized_pipe[0]\n",
    "        return pd.DataFrame(clf['preprocessing'].fit_transform(self.df), columns = naming_features)\n",
    "\n",
    "    def Shapley_feature_importance(self):\n",
    "        if self.optimized_pipe[1] == 0: clf = self.Pipe()\n",
    "        else: clf = self.optimized_pipe[0]\n",
    "        shap.initjs()\n",
    "        dat_trans = self.named_preprocessor()\n",
    "        explainer = shap.TreeExplainer(clf['classifier'].fit(dat_trans, self.df[self.TG])) #,feature_perturbation = \"tree_path_dependent\"\n",
    "        shap_values = explainer.shap_values(dat_trans)\n",
    "        \n",
    "        #### force-plot\n",
    "        a = [_force_plot_html(explainer.expected_value[i], shap_values[i], dat_trans) for i in len(shap_values)]\n",
    "        \n",
    "        ### dependence matrix\n",
    "        ivalues = explainer.shap_interaction_values(dat_trans)\n",
    "        figdm, axdm = plt.subplots(len( dat_trans.columns),  len(dat_trans.columns), figsize=(15, 15))\n",
    "        d = {i: name for i,name in enumerate(dat_trans.columns)}\n",
    "        for i in d.keys():\n",
    "            for j in d.keys():\n",
    "                shap.dependence_plot((d[i], d[j]), ivalues[1], dat_trans, ax = axdm[i,j], show = False)\n",
    "        return (a,  figdm) #fig,\n",
    "\n",
    "#height, width = [500,500]\n",
    "\n",
    "\n",
    "def plotly_cyt(d):\n",
    "    edges = [{'data': {'weight': i['data']['weight'], 'source': str(i['data']['source']), 'target': str(i['data']['target'])}}  for i in d['edges']]\n",
    "    nodes = [{'data': {k:i['data'][k] for k in ('id', 'value', 'name') }, 'position' : dict(zip(('x', 'y'),i['data']['data']))} for i in d['nodes']]\n",
    "    return nodes + edges\n",
    "    \n",
    "def plotly_cyt2(G):\n",
    "    d = nx.cytoscape_data(G)['elements']\n",
    "    pos = nx.spring_layout(G)\n",
    "    edges = [{'data': {'weight': i['data']['weight'], 'source': str(i['data']['source']), 'target': str(i['data']['target'])}}  for i in d['edges']]\n",
    "    nodes = [{'data': {k:i['data'][k] for k in ('id', 'value', 'name') }, 'position' : dict(zip(('x', 'y'),j))} for i,j in zip(d['nodes'], list(pos.values()))]\n",
    "    return nodes + edges\n",
    "\n",
    "def plotly_cyt3(G):\n",
    "    d = nx.cytoscape_data(G)['elements']\n",
    "    pos = nx.spring_layout(G)\n",
    "    edges = [{'data': {'weight': i['data']['weight'], 'source': str(i['data']['source']), 'target': str(i['data']['target'])}}  for i in d['edges']]\n",
    "    nodes = [{'data': {**{k:i['data'][k] for k in ('id', 'value', 'name') }, **{'degree': degree[1]}} , 'position' : dict(zip(('x', 'y'),j))}\n",
    "             for i,j,degree in zip(d['nodes'], list(pos.values()), list(G.degree))]\n",
    "    return nodes + edges\n",
    "\n",
    "def make_colormap_clustering(column, palette, continuous, data):\n",
    "    if not continuous: \n",
    "        lut = dict(zip(sorted(data[column].unique()), sns.color_palette(palette, len(data[column].unique()))))\n",
    "    else: lut = sns.color_palette(palette, as_cmap=True)\n",
    "    return data[column].map(lut)\n",
    "\n",
    "\n",
    "def _force_plot_html(*args):\n",
    "    force_plot = shap.force_plot(*args, matplotlib=False, figsize=(18, 18))\n",
    "    shap_html = f\"<head>{shap.getjs()}</head><body>{force_plot.html()}</body>\"\n",
    "    return html.Iframe(srcDoc=shap_html, height='1800', width='1800',style={\"border\": 0})#\n",
    "\n",
    "def mplfig2html(figure):\n",
    "    pic_IObytes2 = io.BytesIO()\n",
    "    figure.savefig(pic_IObytes2,  format='png')\n",
    "    figure.clear()\n",
    "    pic_IObytes2.seek(0)  \n",
    "    return  html.Img(src ='data:image/png;base64,{}'.format(base64.b64encode(pic_IObytes2.read()).decode())) \n",
    "\n",
    "def mpl2plotlyGraph(figure):\n",
    "    return dcc.Graph(ptools.mpl_to_plotly(figure)) #image_height: int=600,image_width: int=800\n",
    "\n",
    "\n",
    "def convert2cytoscapeJSON(G):\n",
    "    # load all nodes into nodes array\n",
    "    final = {}\n",
    "    final[\"nodes\"] = []\n",
    "    final[\"edges\"] = [] \n",
    "    for node in G.nodes():\n",
    "        nx = {}\n",
    "        nx[\"data\"] = {}\n",
    "        nx[\"data\"][\"id\"] = node\n",
    "        nx[\"data\"][\"label\"] = node\n",
    "        final[\"nodes\"].append(nx.copy())\n",
    "    #load all edges to edges array\n",
    "    for edge in G.edges():\n",
    "        nx = {}\n",
    "        nx[\"data\"]={}\n",
    "        nx[\"data\"][\"id\"]=edge[0]+edge[1]\n",
    "        nx[\"data\"][\"source\"]=edge[0]\n",
    "        nx[\"data\"][\"target\"]=edge[1]\n",
    "        final[\"edges\"].append(nx)\n",
    "    return json.dumps(final)\n",
    "\n",
    "\n",
    "upload_tab = [\n",
    "    dbc.Row(dbc.Col(dbc.Container([\n",
    "        html.H3(\"Upload CSV file\", className=\"display-4\"),\n",
    "        dcc.Dropdown(options =[{'label': 'Include preprocessing', 'value': 1}, \\\n",
    "                               {'label': 'No preprocessing', 'value': 0}], value = 1\n",
    "                              , id='initial_preprocessing'),\n",
    "        dcc.Upload(id='upload_dataset_directly',children=html.Div(['Drag and Drop or ', html.A('Select Files')]),\n",
    "        style={'width': '100%', 'height': '80px',  'lineHeight': '80px', 'font-size': '20px',   'borderWidth': '1px', 'borderStyle': 'dashed',  'borderRadius': '5px',   'textAlign': 'center', 'margin': '10px'},multiple=False),\n",
    "        html.Div(id='direct_dataframe_upload_name'),\n",
    "        html.Div([html.H5(\"Covariates:\", id = 'aaaaa'),\n",
    "                  \n",
    "                  dbc.Col([dcc.Dropdown(options=[],value=[], multi=True, id = 'covariates', placeholder ='Categorical'),\n",
    "                           dcc.Dropdown(options=[],value=[], multi=True, id = 'covariates_cont',placeholder = 'Continuous')]),\n",
    "                 html.H5(\"Variables:\", id = 'aaaab'),\n",
    "                 dcc.Dropdown(options=[],value=[], multi=True, id = 'variables')],\n",
    "                style = {'justify-content': 'center', 'margin-left':'30px'} , hidden = False, id = 'columns_selection'),\n",
    "        \n",
    "        \n",
    "        dbc.Button(\"Send data to Quality Control\", color=\"info\", size = 'lg', className=\"d-grid gap-2\", id='make_qual_control') \n",
    "    ],className=\"h-100 p-5 bg-light border rounded-4 g-0\"), width = 12), justify=\"center\",className=\"h-100 p-5 bg-light border rounded-4 g-0 gap-2\"),\n",
    "    dbc.Col([dcc.Loading(id='Preprocessing_loading',type=\"default\", children= dcc.Tabs([\n",
    "               dcc.Tab(label = 'Box Plots', children = [html.Div([html.H5(\"Covariate:\"),\n",
    "                                                                 dcc.Dropdown(options=[],value=[], multi=False, id = 'covariates_view'),\n",
    "                                                                 html.H5(\"Variables to view:\"),\n",
    "                                                                 dcc.Dropdown(options=[],value=[], multi=True, id = 'variates_view'),\n",
    "                                                                 dbc.Button(\"make violinplot\", color=\"info\", size = 'lg', className=\"d-grid gap-2\", id='make_violin'),\n",
    "                                                                 dcc.Graph(id='boxplots' , style = {'height': '1200px', 'width' : '1500px'})],\n",
    "                                                                 style = {'height': '1500px', 'width' : '2000px','margin-left':'30px'}),\n",
    "                                                        ] ),\n",
    "               dcc.Tab(label = 'Anova Table', children = html.Div( id = 'anova_table_div', style = {'justify-content': 'center'} )),\n",
    "               dcc.Tab(label = 'Explained Variance', children = html.Div(id='EV_table_div') ),\n",
    "               dcc.Tab(label = 'Outliers', children = html.Div(id='outlier_table_div') ),\n",
    "               \n",
    "           ], style = {'justify-content': 'center','display': 'flex' ,'width': '100%','margin-left': '12px','overflow': 'clip'})) ], width=12, style = {'overflow': 'clip'})\n",
    "    \n",
    "]\n",
    "\n",
    "merge_tab = [\n",
    "    dbc.Container([\n",
    "        dbc.Col(html.H2(\"Dataset overview \", className=\"display-4\")),\n",
    "        dbc.Col(html.Div([dbc.Button(\"Download CSV\", color=\"info\", size = 'lg', className=\"d-grid gap-2\", id='btn_csv'),\n",
    "                          dcc.Download(id=\"download-dataframe-csv\")])),\n",
    "        #html.P('Look for parameters that have unexpected behavior, dataset size and other possible concerns with data integrity',className=\"lead\",),\n",
    "        html.Hr(className=\"my-2\"),html.P(\"\"),\n",
    "        dcc.Loading(id=\"loading-1\",type=\"default\", children=html.Div(id='Merged_df', style = {'justify-content': 'center', 'margin': '0 auto', 'width': '95%'} ) )\n",
    "    ],className=\"h-100 p-5 bg-light border rounded-3 g-0\", fluid = True),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "kep_tab=[ dbc.Row([\n",
    "           dbc.Col(\n",
    "               [dbc.Row([\n",
    "                   dbc.Container([ \n",
    "                       html.H5(\"what are the continous columns for the UMAP?\", id = 'kep_tab_continuous_columns_target'), \n",
    "                       dbc.Popover([ dbc.PopoverHeader(\"how we look at continuous data\"),dbc.PopoverBody(\"https://umap-learn.readthedocs.io/en/latest/basic_usage.html\")],target=\"kep_tab_continuous_columns_target\",trigger=\"hover\",),\n",
    "                       dcc.Dropdown(options=[],value=[], multi=True, id = 'UMAP_cont'),\n",
    "                       html.H5(\"what are the categorical columns for the UMAP?\", id = 'kep_tab_cat_columns_target'),\n",
    "                       dbc.Popover([ dbc.PopoverHeader(\"how we look at categorical data\"),dbc.PopoverBody(\"see https://umap-learn.readthedocs.io/en/latest/composing_models.html#diamonds-dataset-example\")],target=\"kep_tab_cat_columns_target\",trigger=\"hover\",),\n",
    "                       dcc.Dropdown(options=[],value=[], multi=True, id = 'UMAP_cat'),\n",
    "                       html.H5(\"Do you want to fit the UMAP to a feature?\", id = 'keep_tab_metric_learn'), #https://umap-learn.readthedocs.io/en/latest/supervised.html\n",
    "                       dbc.Popover([ dbc.PopoverHeader(\"fitting umap to feature\"),dbc.PopoverBody(\"https://umap-learn.readthedocs.io/en/latest/supervised.html\")],target=\"keep_tab_metric_learn\",trigger=\"hover\",),\n",
    "                       dcc.Dropdown(options=[],value=[], multi=False, id = 'UMAP_y'),\n",
    "                       html.H5(\"How many neighboors for the UMAP to use?\", id = 'keep_tab_nneighboors'),\n",
    "                       dbc.Popover([ dbc.PopoverHeader(\"n neighboors parameter\"),dbc.PopoverBody(\"This parameter controls how UMAP balances local versus global structure in the data. It does this by \\\n",
    "                       constraining the size of the local neighborhood UMAP will look at when attempting to learn the manifold structure of the data. \\\n",
    "                       This means that low values of n_neighbors will force UMAP to concentrate on very local structure (potentially to the detriment of the big picture),\\\n",
    "                       while large values will push UMAP to look at larger neighborhoods of each point when estimating the manifold structure of the data, \\\n",
    "                       losing fine detail structure for the sake of getting the broader of the data. _ see https://umap-learn.readthedocs.io/en/latest/parameters.html#n-neighbors\")],target=\"keep_tab_nneighboors\",trigger=\"hover\",),\n",
    "                       dbc.Input(id=\"n_neighboors\", type=\"number\", value = 15, min = 10, max = 1000), #https://umap-learn.readthedocs.io/en/latest/parameters.html#n-neighbors\n",
    "                       html.H5('Type of scaling to use:', id= 'kep_tab_scale'),\n",
    "                       dbc.Popover([ dbc.PopoverHeader(\"Should I scale my data?\"),dbc.PopoverBody(\"The default answer is yes, but, of course, the real answer is “it depends”. \\\n",
    "                       If your features have meaningful relationships with one another (say, latitude and longitude values) then normalising per feature is not a good idea. \\\n",
    "                       For features that are essentially independent it does make sense to get all the features on (relatively) the same scale. \\\n",
    "                       The best way to do this is to use pre-processing tools from scikit-learn. All the advice given there applies as sensible preprocessing for UMAP,\\\n",
    "                       and since UMAP is scikit-learn compatible you can put all of this together into a scikit-learn pipeline.\")],target=\"kep_tab_scale\",trigger=\"hover\",),\n",
    "                       dbc.RadioItems(id=\"UMAP_radio\",\n",
    "                        options=[\n",
    "                            {\"label\": \"No Standardization\", \"value\": 1},\n",
    "                            {\"label\": \"Standard scaler\", \"value\": 2},\n",
    "                            {\"label\": \"Pipeline from machine learning tab\",\"value\": 3}],value = 2,\n",
    "                            labelCheckedStyle={\"color\": \"#223c4f\", 'font-size': '14px'},\n",
    "                            labelStyle = {}, style = {'font-size': '14px', 'margin' : '8px', 'margin-left': '20px' ,'transform':'scale(1.1)'}, switch=True,\n",
    "                            inputStyle = { }          \n",
    "                                     ),\n",
    "                      dbc.Button(\"Generate UMAP\", color=\"info\", size = 'lg', className=\"d-grid gap-2\", id='UMAP_start') ],className=\"h-100 p-5 bg-light border rounded-3 g-0 d-grid\", fluid = True),\n",
    "                      dbc.Popover([ dbc.PopoverHeader(\"what is UMAP?\"),dbc.PopoverBody(\"see https://umap-learn.readthedocs.io/en/latest/how_umap_works.html \\nhttps://umap-learn.readthedocs.io/en/latest/scientific_papers.html\\nhttps://umap-learn.readthedocs.io/en/latest/faq.html#what-is-the-difference-between-pca-umap-vaes\")],target=\"UMAP_start\",trigger=\"hover\",),\n",
    "                   \n",
    "                   \n",
    "               ])],width=2)  , \n",
    "           dbc.Col([dcc.Loading(id=\"loading-umap\",type=\"default\", children= dcc.Tabs([\n",
    "               dcc.Tab(label = 'umap-view', children = [html.Div(dcc.Graph(id='UMAP_view'), style = {'height': '1000px', 'width' : '1500px','margin-left':'30px'}),html.Div( id = 'umap_selected_stats', style = {'width': '98%'})] ),\n",
    "               dcc.Tab(label = 'heatmap/cytoscape', children = html.Div( id = 'cytoscape', style = {'justify-content': 'center'} )),\n",
    "               dcc.Tab(label = 'hdbscan clustering', children = html.Div(id='graph') ),\n",
    "               \n",
    "           ], style = {'justify-content': 'center','display': 'flex' ,'width': '100%','margin-left': '12px','overflow': 'clip'})) ], width=10, style = {'overflow': 'clip'})],  className=\"g-0\")] #\n",
    "                                \n",
    "#className=\"nav nav-pills\"      , className=\"g-0\"         autosize=False                 \n",
    "\n",
    "time_series_tab = [\n",
    "    dbc.Row([\n",
    "        dbc.Col( dbc.Container([\n",
    "            html.H5(\"Target column\"), \n",
    "            dcc.Dropdown(options=[],value=[], multi=False, id = 'prophet_y'),\n",
    "            html.H5(\"Datetime column\"), \n",
    "            dcc.Dropdown(options=[],value=[], multi=False, id = 'prophet_ds'),\n",
    "            html.Hr(style= {'margin-bottom': '3px'}),\n",
    "            html.H5(\"Additional regressors\"),\n",
    "            dcc.Dropdown(options=[],value=[], multi=True, id = 'prophet_regressors'),\n",
    "            html.Hr(style= {'margin-bottom': '3px'}),\n",
    "            html.H5('Rolling average'),\n",
    "            html.H6('number of days'),\n",
    "            dbc.Input(id=\"prophet_rolling_average\", type=\"number\", value = 0, min = 0, max = 366, step = 0.25),\n",
    "            html.Hr(style= {'margin-bottom': '3px'}),\n",
    "            html.H5(\"Growth\"), \n",
    "            dcc.Dropdown(options=[\n",
    "                {\"label\": \"logistic\", \"value\": 'logistic'},\n",
    "                {\"label\": \"flat\", \"value\": 'flat'},\n",
    "                {\"label\": \"linear\", \"value\": 'linear'}\n",
    "            ],value='linear', multi=False,id = 'prophet_growth'),\n",
    "            html.H5(\"Target maximum value\"),\n",
    "            dbc.Input(id=\"prophet_cap\", type=\"number\", value = 1, step = .01),\n",
    "            html.H5(\"Target minimum value\"),\n",
    "            dbc.Input(id=\"prophet_floor\", type=\"number\", value = 0, step = .01),\n",
    "            html.Hr(style= {'margin-bottom': '3px'}),\n",
    "            html.H5('Seasonnality'),\n",
    "            html.H6('frequency'),\n",
    "            dbc.Checklist( options = [\n",
    "                {\"label\": \"Yearly\", \"value\": 'yearly_seasonality'},\n",
    "                {\"label\": \"Weekly\", \"value\": 'weekly_seasonality'},\n",
    "                {\"label\": \"Daily\", \"value\": 'daily_seasonality'},\n",
    "            ]  ,value=['yearly_seasonality'], id = 'prophet_seasonality' , \n",
    "                          style = {'font-size': '14px', 'margin' : '2px', 'margin-left': '20px' ,'transform':'scale(1.)'}, switch=True),\n",
    "            html.H6('mode'),\n",
    "            dcc.Dropdown(options=[\n",
    "                {\"label\": \"additive\", \"value\": 'additive'},\n",
    "                {\"label\": \"multiplicative\", \"value\": 'multiplicative'}\n",
    "            ], multi=False,id = 'seasonality_mode', value = 'additive'),\n",
    "            html.H6('scale'),\n",
    "            dbc.Input(id=\"season_prior\", type=\"number\", value = 10, min = 1, max = 100),\n",
    "            html.Hr(style= {'margin-bottom': '3px'}),\n",
    "            html.H5('Change points'),\n",
    "            html.H6('quantity'),\n",
    "            dbc.Input(id=\"prophet_n_change_points\", type=\"number\", value = 25, min = 0, max = 100,step =1),\n",
    "            html.H6('scale'),\n",
    "            dbc.Input(id=\"changepoint_prior\", type=\"number\", value = .05, min = 0, max = 10., step = 0.01),\n",
    "            html.H6('range'),\n",
    "            dbc.Input(id=\"changepoint_range\", type=\"number\", value = .8, min = 0.1, max = 1., step = 0.01),\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ],className=\"h-100 p-5 bg-light border rounded-3 g-0\", fluid = True), width = 2),\n",
    "        dbc.Col(dcc.Loading(id=\"loading-prophet\",type=\"default\", children=html.Div(id='prophet_plots', style = {'justify-content': 'center', 'margin': '0 auto', 'width': '100%'} ), style= {'margin-top': '100px'})),\n",
    "        dbc.Col( dbc.Container([ \n",
    "            html.H5('Forecast'),\n",
    "            html.H6('prediction range'),\n",
    "            dcc.DatePickerRange(id= 'prophet_future_dates', display_format='MMM DD YYYY'),\n",
    "            html.Hr(style= {'margin-bottom': '50px'}),\n",
    "            html.H6('remove these month'),\n",
    "            dcc.Dropdown(options=[ {\"label\":  calendar.month_name[num], \"value\": num} for num in range(1,12)],value=[], multi=True,id = 'prophet_remove_months'),\n",
    "            html.H6('remove these days of the week'),\n",
    "            dcc.Dropdown(options=[ {\"label\":  day_name, \"value\": num} for num,day_name in enumerate(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])],\n",
    "                         value=[], multi=True,id = 'prophet_remove_days_of_the_week'),\n",
    "            html.H6('remove these hours of the day'),\n",
    "            dcc.Dropdown(options=[ {\"label\":  str(num)+':00-'+str(num+1)+':00', \"value\": num} for num in range(0,24)],value=[], multi=True,id = 'prophet_remove_hours'),\n",
    "            html.Hr(style= {'margin-bottom': '50px'}),\n",
    "            dbc.Button(\"Run forecast\", color=\"info\", size = 'lg', id='run_prophet')\n",
    "        ],className=\"h-100 p-5 bg-light border rounded-3 g-0\", fluid = True), className=\"g-0\", width = 2)\n",
    "        \n",
    "            \n",
    "    ], className=\"g-0\", style={'margin-bottom': '10px'})\n",
    "    \n",
    "    \n",
    "]\n",
    "\n",
    "sklearn_preprocessor_list = ['Binarizer','FunctionTransformer', 'KBinsDiscretizer', 'KernelCenterer', 'LabelBinarizer', 'LabelEncoder', 'MinMaxScaler',\n",
    "                             'MaxAbsScaler','QuantileTransformer', 'Normalizer', 'OneHotEncoder', 'OrdinalEncoder', 'PowerTransformer', 'RobustScaler', 'SplineTransformer',\n",
    "                              'StandardScaler', 'PolynomialFeatures']\n",
    "sklearn_decomposition_list = ['DictionaryLearning','FastICA', 'IncrementalPCA', 'KernelPCA', 'MiniBatchDictionaryLearning', 'MiniBatchSparsePCA', \n",
    "                              'NMF','PCA','SparsePCA', 'FactorAnalysis','TruncatedSVD', 'LatentDirichletAllocation']\n",
    "sklearn_manifold_list = ['LocallyLinearEmbedding', 'Isomap', 'MDS', 'SpectralEmbedding', 'TSNE']\n",
    "transformers = sklearn_preprocessor_list + sklearn_decomposition_list + sklearn_manifold_list + ['UMAP', 'passthrough']\n",
    "transformer_options = [ {'label': x, 'value': x } for x in  transformers] \n",
    "\n",
    "sklearn_ensemble_list  = ['BaseEnsemble','RandomForestClassifier', 'RandomForestRegressor', 'RandomTreesEmbedding', 'ExtraTreesClassifier', 'ExtraTreesRegressor',\n",
    "                          'BaggingClassifier', 'BaggingRegressor', 'IsolationForest', 'GradientBoostingClassifier', 'GradientBoostingRegressor', 'AdaBoostClassifier',\n",
    "                          'AdaBoostRegressor', 'VotingClassifier', 'VotingRegressor', 'StackingClassifier', 'StackingRegressor', 'HistGradientBoostingClassifier',\n",
    "                          'HistGradientBoostingRegressor']\n",
    "sklearn_linear_model_list = ['ARDRegression', 'BayesianRidge', 'ElasticNet', 'ElasticNetCV', 'Hinge', 'Huber', 'HuberRegressor', 'Lars', 'LarsCV', 'Lasso', 'LassoCV',\n",
    "                             'LassoLars', 'LassoLarsCV', 'LassoLarsIC', 'LinearRegression', 'Log', 'LogisticRegression', 'LogisticRegressionCV', 'ModifiedHuber',\n",
    "                             'MultiTaskElasticNet', 'MultiTaskElasticNetCV', 'MultiTaskLasso', 'MultiTaskLassoCV', 'OrthogonalMatchingPursuit', 'OrthogonalMatchingPursuitCV',\n",
    "                             'PassiveAggressiveClassifier', 'PassiveAggressiveRegressor', 'Perceptron', 'QuantileRegressor', 'Ridge', 'RidgeCV', 'RidgeClassifier', \n",
    "                             'RidgeClassifierCV', 'SGDClassifier', 'SGDRegressor', 'SGDOneClassSVM', 'SquaredLoss', 'TheilSenRegressor', \n",
    "                            'RANSACRegressor', 'PoissonRegressor', 'GammaRegressor', 'TweedieRegressor']\n",
    "sklearn_naive_bayes_list = ['BernoulliNB', 'GaussianNB', 'MultinomialNB', 'ComplementNB', 'CategoricalNB']\n",
    "classifier_list = ['LGBMClassifier', 'LGBMRegressor'] + sklearn_ensemble_list + sklearn_naive_bayes_list + sklearn_linear_model_list\n",
    "classifier_options = [ {'label': x, 'value': x } for x in  classifier_list] \n",
    "\n",
    "\n",
    "ML_tab = [\n",
    "   dbc.Row([\n",
    "       dbc.Col(\n",
    "           [dbc.Container([ \n",
    "                dbc.Row([\n",
    "                   dbc.Col([ html.H5(\"number of transformers:\")]), \n",
    "                   dbc.Col([#dcc.Dropdown(options=[ {'label': str(x), 'value': str(x)} for x in range(10)],value='2', multi=False,clearable=False, id = 'n_tabs')\n",
    "                            dbc.Input(id=\"n_tabs\", type=\"number\", value = 1, min = 1, max = 10)\n",
    "                           ]),\n",
    "                   dbc.Col([html.H5(\"Target:\")]),\n",
    "                   dbc.Col([dcc.Dropdown(options=[],value=[], multi=False, id = 'ML_target',clearable=False)]),\n",
    "                   dbc.Col([html.H5(\"Classifier:\", id = 'ml_tab_classifier'), dbc.Popover([ dbc.PopoverHeader(\"chosing a classifier\"),dbc.PopoverBody('see: \\\n",
    "                   https://scikit-learn.org/stable/supervised_learning.html#supervised-learning\\n https://lightgbm.readthedocs.io/en/latest/Quick-Start.html ')],target=\"ml_tab_classifier\",trigger=\"hover\",)]),\n",
    "                   dbc.Col([dcc.Dropdown(options=classifier_options ,value = 'LGBMClassifier',  multi=False, id = 'clf_disp', clearable=False)]) ]), #)],className=\"h-100 p-5 bg-light border rounded-3 g-0\", fluid = True),  \n",
    "            dbc.Row([dbc.Col( \n",
    "                   [html.H5(\"Columns to be transformed:\")] +\n",
    "                   [ dcc.Dropdown(options= ['0'], value = ['0'],multi=True,clearable=False, id = 'Columns_'+ str(i))  for i in range(3)], id = 'preprocessing_columns'),\n",
    "            dbc.Col(\n",
    "                   [html.H5(\"Column transformers:\", id = 'ml_tab_column_trans')] + #https://scikit-learn.org/stable/modules/preprocessing.html#\n",
    "                   [ dcc.Dropdown(options= transformer_options, value = ['passthrough'], multi=True,clearable=False, id = 'ColumnTransformer_'+ str(i))  for i in range(3)], id = 'preprocessing_functions'),\n",
    "            dbc.Popover([ dbc.PopoverHeader(\"preprocessing the data\"),dbc.PopoverBody(\"see:\\n https://scikit-learn.org/stable/modules/preprocessing.html\\n\\\n",
    "                   https://scikit-learn.org/stable/modules/decomposition.html#decompositions#\\nhttps://scikit-learn.org/stable/modules/clustering.html#clustering\")],target=\"ml_tab_column_trans\",trigger=\"hover\",)\n",
    "                                   ])],className=\"h-100 p-5 bg-light border rounded-1 g-0\",fluid = True)\n",
    "\n",
    "\n",
    "           ],width=6, id='ml_user_input') ] + [dbc.Col([dbc.Button(\"Update Pipeline\", color=\"info\", size = 'lg', className=\"d-grid gap-2\", id='submit_pipe'),\n",
    "                                                         html.Div(id = 'show_pipeline', style ={'width': '50%','borderWidth': '0px' ,'border': 'white'})], \n",
    "                                                        width = 6)], className=\"g-0\",justify=\"center\", style = {'font-size': '12px', 'margin' : '5px' }),\n",
    "    dbc.Row([dbc.Col(\n",
    "        dbc.Container([\n",
    "           dbc.Row([ html.H2(\"Testing the pipeline\", style ={'margin': '20px'})]), #,justify=\"center\"\n",
    "            dbc.Row([dbc.Col([html.H5(\"Number of runs for hyperparameter optimization (use < 10 for no optimization):\", id = 'ml_tab_tunning')], width = 3),\n",
    "                      dbc.Popover([ dbc.PopoverHeader(\"Tunning the model\"),dbc.PopoverBody(\"here we use scikit optimize's bayesian optimization to tune the hyperparameters\\\n",
    "                      https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html\")],target=\"ml_tab_tunning\",trigger=\"hover\",),\n",
    "                    dbc.Col([dbc.Input(id=\"slider_hyperopt\", type=\"number\", value = 5, min = 10, max = 1000)], width = 1)], className=\"g-0\", style={'margin-bottom': '10px'}), #\n",
    "            dbc.Row([dbc.Button(\"Run pipeline\", color=\"info\", size = 'lg', className=\"d-grid gap-2\",  id='run_ML')], className = 'd-grid g-0'), \n",
    "            dcc.Loading(id=\"loading-ml\",type=\"default\", children=html.Div(id = 'ml_results', style = {'justify-content': 'center', 'margin': '0 auto', 'width': '1760', 'height' : '220px'}), \n",
    "                                 style= {'margin-top': '-300px','justify-content': 'center'})],className=\"h-100 p-5 bg-light border rounded-3 g-0\", fluid = True)  \n",
    "                     , width = 12,  style = {'justify-content': 'center', 'height' : '220px'}) ], className=\"g-0\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gwas_options = glob('gwas/*.mlma')\n",
    "\n",
    "\n",
    "GWAS_tab=[ dbc.Row([\n",
    "           dbc.Col(\n",
    "               [dbc.Row([\n",
    "                   dbc.Container([ \n",
    "                       html.H5(\"which feature GWAS do you want to visualize?\", id = 'gwaslabel1'), \n",
    "                       dcc.Dropdown(options=[{'label': x.split('.')[0].split('/')[-1], 'value':x} for x in gwas_options],\\\n",
    "                                    value=[], multi=False, id = 'GWAS_options'),\n",
    "                       html.H5(\"what is the pvalue threshold?\", id = 'pval_threshold_value'),\n",
    "                       dcc.Input(value = 5.7, type=\"number\", debounce = True,id = 'pvalue_threshold'),\n",
    "                       dbc.Button(\"visualize GWAS\", color=\"info\", size = 'lg', className=\"d-grid gap-2\", id='View GWAS') ],\\\n",
    "                       className=\"h-100 p-5 bg-light border rounded-3 g-0 d-grid\", fluid = True),\n",
    "                      \n",
    "                   \n",
    "               ])],width=2)  , \n",
    "           dbc.Col([dcc.Loading(id=\"loading gwas\",type=\"default\", children= dcc.Tabs([\n",
    "               dcc.Tab(label = 'gwastab', children = [html.Div(dcc.Graph(id='gwastaviewer',\n",
    "                                                                 style = {'height': '1000px', 'width' : '1500px','margin-left':'30px'})),\n",
    "                                                        html.Div( id = 'part2gwastable', style = {'width': '98%'})] ),\n",
    "               dcc.Tab(label = 'igvtab', children = html.Div([dcc.Dropdown(options = [], value=[], multi=False, id = 'igv_options'),\n",
    "                                                                dcc.Loading(children = html.Div(id='igvGraph'), id ='igvloading')],\n",
    "                                                                style = {'height': '1000px', 'width' : '1500px','margin-left':'30px'},\n",
    "                                                                id='igvdiv' ) ),\n",
    "               dcc.Tab(label = 'pileuptab', children = html.Div([dcc.Dropdown(options = [], value=[], multi=False, id = 'pileup_options'),\n",
    "                                                                dcc.Loading(children = html.Div(id='pileupGraph'), id ='pileuploading')],\n",
    "                                                                style = {'height': '1000px', 'width' : '1500px','margin-left':'30px'},\n",
    "                                                                id='pileupdiv' ) ),\n",
    "               \n",
    "           ], style = {'justify-content': 'center','display': 'flex' ,'width': '100%','margin-left': '12px','overflow': 'clip'})) ], \n",
    "                   width=10, style = {'overflow': 'clip'})],  className=\"g-0\")] #\n",
    "                               \n",
    "\n",
    "    \n",
    "\n",
    "# html.Iframe(srcDoc = ret_map._repr_html_().decode(), height='1280', width='2350') iframe for html representation of pipeline sklearn\n",
    "tab_style = {\n",
    "    \"background\": \"#223c4f\",\n",
    "    'color': \"#6cc3d5\",\n",
    "    'text-transform': 'lowercase',\n",
    "    'border': '#223c4f',\n",
    "    'font-size': '12px',\n",
    "    'font-weight': 100, #200\n",
    "    'align-items': 'center',\n",
    "    'justify-content': 'center',\n",
    "    'border-radius': '0px',\n",
    "    'height': '20px',\n",
    "    'display': 'flex',\n",
    "\n",
    "    #'padding':'6px'\n",
    "}\n",
    "\n",
    "tab_selected_style = {\n",
    "    \"background\": \"#153751\",\n",
    "    'color': 'white',\n",
    "    'text-transform': 'uppercase',\n",
    "    'font-size': '12px',\n",
    "    'font-weight': 100, #200\n",
    "    'align-items': 'center',\n",
    "    'height': '20px',\n",
    "    'justify-content': 'center',\n",
    "    'display': 'flex',\n",
    "    #'box-shadow': '60px 0 #223c4f, -60px 0 solid #223c4f',\n",
    "    'border-style': 'solid #223c4f',\n",
    "    'border-color': '#223c4f',\n",
    "    'border-width': '0',\n",
    "    #'border-radius': '50px'\n",
    "}\n",
    "\n",
    "subtab_style = {'height': '20px','display': 'flex','font-size': '12px'}\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dbc.NavbarSimple([], brand = 'GWASdashbord - Palmer Laboraty UCSD', brand_style ={'color': \"white\",'font-size': '14px'} ,\n",
    "                     style = { 'align-items': 'left','justify-content': 'left', 'font-size': '14px', 'height': '32px'},\n",
    "                    color = \"#223c4f\"),\n",
    "    dcc.Store(id='df_no_preprocess', storage_type='memory'), #storage_type='local'\n",
    "    dcc.Store(id='df', storage_type='memory'),\n",
    "    dcc.Store(id='df_with_umap', storage_type='memory'),\n",
    "    dcc.Store(id='umap_select_columns', storage_type='memory'),\n",
    "    dcc.Store(id='selected_points_umap', storage_type='memory'), #html.Table(id='all_dfs')    selected_points_umap\n",
    "    dcc.Tabs([\n",
    "        dcc.Tab(label = 'Dataset', children = upload_tab , style=tab_style, selected_style=tab_selected_style),\n",
    "        dcc.Tab(label = 'Quality Control', children = merge_tab , style=tab_style, selected_style=tab_selected_style),\n",
    "        dcc.Tab(label='Exploratory Data Analysis', children=kep_tab, style=tab_style, selected_style=tab_selected_style),\n",
    "        dcc.Tab(label='Time Series', children=time_series_tab, style=tab_style, selected_style=tab_selected_style),\n",
    "        dcc.Tab(label='Machine Learning', children=ML_tab, style=tab_style, selected_style=tab_selected_style),\n",
    "        dcc.Tab(label='GWAS', children=GWAS_tab, style=tab_style, selected_style=tab_selected_style)]) , #,className=\"nav nav-pills\"\n",
    "        ])\n",
    "\n",
    "def get_cq_list(x, df):\n",
    "    a = df[df.Sample ==  x].Cq.values\n",
    "    return [60 if np.isnan(x) else x for x in a]\n",
    "\n",
    "def get_det_list(x, df):\n",
    "    a = df[df.Sample ==  x].Call.values\n",
    "    return [1 if x=='(+) Positive' else 0 for x in a]\n",
    "\n",
    "def FIND_Better(row, column, df):\n",
    "    series = df[df.index == str(row['SiteID'])][column]\n",
    "    if series.shape[0] == 0: return -1\n",
    "    return series.iloc[0]\n",
    "cyto.load_extra_layouts()\n",
    "        \n",
    "@app.callback(Output(component_id= 'df_no_preprocess', component_property ='data'),\n",
    "              Output(component_id= 'direct_dataframe_upload_name', component_property = 'children'),\n",
    "              Input('upload_dataset_directly', 'contents'),\n",
    "              State('upload_dataset_directly', 'filename'),\n",
    "              State('upload_dataset_directly', 'last_modified'))\n",
    "def merge_csv_update_spreadsheet(up_content, up_filename,  up_date ): #qpcr,\n",
    "    ctx = dash.callback_context.triggered[0]['prop_id'].split('.')[0]\n",
    "    if up_content != None and ctx == 'upload_dataset_directly': \n",
    "        content_type, content_string = up_content.split(',')\n",
    "        decoded = base64.b64decode(content_string)\n",
    "        if '.csv' in up_filename:\n",
    "            final = pd.read_csv(io.StringIO(decoded.decode('utf-8')))\n",
    "        elif '.tsv' in up_filename:\n",
    "            final = pd.read_csv(io.StringIO(decoded.decode('utf-8')), sep='\\t')\n",
    "            \n",
    "        else: return html.Div(), html.Div([  html.H6(up_filename), html.Hr() ])  #minimal=True\n",
    "        return  final.to_json(), html.Div([html.H6(up_filename), html.Hr()]) #2350 interactions=None, \n",
    "    \n",
    "    else: return html.Div(), html.Div()\n",
    "\n",
    "def list2options(l):\n",
    "    return [{'label': x, 'value': x} for x in l]\n",
    "\n",
    "@app.callback(Output(\"download-dataframe-csv\", \"data\"),\n",
    "             Input(\"btn_csv\", \"n_clicks\"),\n",
    "             State('upload_dataset_directly', 'filename'),\n",
    "             State('df', 'data'),\n",
    "             prevent_initial_call=True)\n",
    "def download_csv(n_clicks,filename, data):\n",
    "    df = pd.read_json(data,convert_dates = False)\n",
    "    return dcc.send_data_frame(df.to_csv, f\"qc_{filename}.csv\")\n",
    "\n",
    "    \n",
    "@app.callback(Output('covariates', 'options'), Output('covariates', 'value'),\n",
    "              Output('covariates_cont', 'options'), Output('covariates_cont', 'value'),\n",
    "              Output('variables', 'options'), Output('variables', 'value'),\n",
    "              Input('df_no_preprocess', 'data'))\n",
    "def preprocessing_options(data):\n",
    "    try: \n",
    "        df = pd.read_json(data,convert_dates = False)\n",
    "        stR = statsReport.stat_check(df)\n",
    "        return list2options(df.columns), stR.categorical_columns,\\\n",
    "               list2options(df.columns), stR.numerical_columns,\\\n",
    "               list2options(df.columns), stR.numerical_columns\n",
    "               \n",
    "    \n",
    "        \n",
    "    except: \n",
    "        return [], [], [],[], [], []\n",
    "        \n",
    "\n",
    "@app.callback(Output('columns_selection', 'hidden'),\n",
    "            Input('initial_preprocessing', 'value'))  \n",
    "def hide_col_selection(val):\n",
    "    if val: return False\n",
    "    return True\n",
    "\n",
    "@app.callback(Output('boxplots', 'figure'),\n",
    "              Input('make_violin', 'n_clicks'),\n",
    "              State('variates_view', 'value'),\n",
    "              State('covariates_view', 'value'),\n",
    "              State('df', 'data'),\n",
    "              prevent_initial_call=True)\n",
    "def make_boxplot(click, var,cov, data):\n",
    "    try: df = pd.read_json(data)\n",
    "    except: return  px.scatter(x = [0], y = [0])\n",
    "    stR = statsReport.stat_check(df)\n",
    "    melted = stR.var_distributions_for_plotly(targets=var, covariates = [cov])\n",
    "    figdist = px.violin(melted,  x = 'variable', y = 'normalized values', \n",
    "                        color=cov, box=True, points=\"all\", hover_data=melted.columns)# \n",
    "    return figdist\n",
    "\n",
    "\n",
    "@app.callback(Output('Merged_df','children'),\n",
    "            Output('df', 'data'),\n",
    "            Output('anova_table_div', 'children'),\n",
    "            Output('EV_table_div', 'children'),\n",
    "            Output('outlier_table_div', 'children'),\n",
    "            Output('variates_view', 'options'), Output('covariates_view', 'options'),\n",
    "            Input('make_qual_control', 'n_clicks'),\n",
    "            State('initial_preprocessing', 'value'),\n",
    "            State('df_no_preprocess', 'data'),\n",
    "            State('covariates', 'value'),\n",
    "            State('covariates_cont', 'value'),\n",
    "            State('variables', 'value'))  \n",
    "def preprocess_data(nclicks,prep, data, categorical, covariates_num, variables):\n",
    "    try: df = pd.read_json(data)\n",
    "    except: return html.Div(), html.Div(), html.Div(), html.Div(), html.Div(), [],[]\n",
    "    if prep:\n",
    "        stR = statsReport.stat_check(df)\n",
    "        anovatable = stR.do_anova(targets=variables, covariates = categorical).round(3)\n",
    "        melted = stR.var_distributions_for_plotly(targets=variables, covariates = categorical)\n",
    "        #figdist = px.violin(melted,  x = 'variable', y = 'normalized values', \n",
    "        #                    color=categorical[0], box=True, points=\"all\", hover_data=df.columns)\n",
    "        dfohe = df.copy()\n",
    "        ohe = OneHotEncoder()\n",
    "        oheencoded = ohe.fit_transform(dfohe[categorical]).todense()\n",
    "        dfohe[[f'OHE_{x}'for x in ohe.get_feature_names(categorical)]] = oheencoded\n",
    "        explained_variances = statsReport.stat_check(dfohe).explained_variance(variables, \n",
    "                                                                               list(dfohe.columns[dfohe.columns.str.contains(r'|'.join(['OHE']+covariates_num))]))\n",
    "        melted_variances = explained_variances.reset_index().melt(id_vars = ['index'], \n",
    "                                                          value_vars=explained_variances.columns[1:])\\\n",
    "                                                          .rename({'index':'group'}, axis =1 ).query('value > 0.02')\\\n",
    "                                                          .sort_values('value', ascending=False)\n",
    "        melt_list = pd.DataFrame(melted_variances.groupby('variable')['group'].apply(list)).reset_index()\n",
    "        aaaa = melt_list.apply(lambda x: statsReport.regress_out(dfohe,[x.variable],   x.group), axis =1)\n",
    "        resid_dataset = pd.concat(list(aaaa), axis = 1)#.add_prefix('resid_')\n",
    "        cols2norm = resid_dataset.columns[resid_dataset.columns.str.contains('regressedLR_')]\n",
    "        resid_dataset[cols2norm] = statsReport.quantileTrasformEdited(resid_dataset, cols2norm)\n",
    "        df = pd.concat([df,resid_dataset],axis = 1)\n",
    "        anovatable = anovatable.applymap(lambda x: round(x, 3) if x != 'not enough groups' else 'NA').reset_index()\n",
    "        anovatable.columns = [\"_\".join(pair) for pair in anovatable.columns]\n",
    "        anovatable = anovatable.loc[:,~anovatable.columns.str.contains('qvalue') ]\n",
    "        anovatablegraph =  dcc.Graph(figure=ff.create_table(anovatable))\n",
    "        expvargraph = dcc.Graph(figure=ff.create_table(melted_variances))\n",
    "        \n",
    "\n",
    "    else: \n",
    "        anovatablegraph =  dcc.Graph(figure=ff.create_table(pd.DataFrame()))\n",
    "        expvargraph = dcc.Graph(figure=ff.create_table(pd.DataFrame()))\n",
    "        \n",
    "    stR = statsReport.stat_check(df)\n",
    "    outliertablegraph =  dcc.Graph(figure=ff.create_table(stR.get_outliers(subset = variables)))\n",
    "    \n",
    "    htmrep = [ html.Iframe(srcDoc = ProfileReport(df, correlations=None,interactions=None).to_html(), height='900', width='1600')] #\n",
    "    \n",
    "    return htmrep, df.to_json(), anovatablegraph, \\\n",
    "           expvargraph, outliertablegraph,\\\n",
    "           list2options(df.columns), list2options(df.columns)\n",
    "    \n",
    "    \n",
    "classifier_list = ['LGBMClassifier', 'LGBMRegressor'] + sklearn_ensemble_list + sklearn_naive_bayes_list + sklearn_linear_model_list\n",
    "    \n",
    "def inpt_children_to_pipe(columns, funcs, classif):\n",
    "    C = [x['props']['value'] for x in columns[1:]]\n",
    "    F = [x['props']['value'] for x in funcs[1:]]\n",
    "    \n",
    "    if classif == 'LGBMClassifier' or  classif == 'LGBMRegressor':\n",
    "        #classifier_function = getattr(lightgbm, classif)(boosting_type='gbdt',  subsample=1.0) #boosting_type='gbdt', bagging_fraction = 0\n",
    "        classifier_function = globals()[classif](boosting_type='gbdt',  subsample=1.0)\n",
    "    else: classifier_function = globals()[classif]()\n",
    "    return Pipeline(steps = [('preprocessing', make_pipe(C, F)), ('classifier', classifier_function)])\n",
    "        \n",
    "def make_pipe(columns_list, transformer_list):\n",
    "    simplfy = []\n",
    "    for num, (cols, trans) in enumerate(zip(columns_list, transformer_list) ): \n",
    "        sub_smp = []\n",
    "        for x in trans:\n",
    "            if x[0].isupper() == True:\n",
    "                if x in sklearn_decomposition_list: sub_smp += [globals()[x](n_components = 2)]\n",
    "                else: sub_smp += [globals()[x]()]\n",
    "            else: sub_smp += [x]\n",
    "        simplfy += [tuple([str(num), make_pipeline(*sub_smp), tuple(cols)])]\n",
    "    return ColumnTransformer(simplfy)\n",
    "    #simplfy =[ tuple([str(num), make_pipeline(*[locals()[x]() if x[0].isupper() == True else x for x in trans ]), tuple(cols)]) for num, (cols, trans) in enumerate(zip(columns_list, transformer_list) )]\n",
    "    #return ColumnTransformer(simplfy)\n",
    "    \n",
    "\n",
    "@app.callback(Output(component_id= 'ml_results', component_property ='children'),\n",
    "              Input(component_id = 'run_ML', component_property = 'n_clicks'),\n",
    "              State(component_id= 'preprocessing_functions', component_property ='children'),\n",
    "              State(component_id= 'preprocessing_columns', component_property ='children'),\n",
    "              State(component_id = 'clf_disp', component_property = 'value'),\n",
    "              State(component_id = 'df', component_property = 'data'),\n",
    "              State(component_id = 'ML_target', component_property = 'value'),\n",
    "              State(component_id = 'slider_hyperopt', component_property = 'value')) \n",
    "def run_ML(clicked, f_list, c_list, val, data, target, ncalls):\n",
    "    pipe = inpt_children_to_pipe(c_list,f_list, val)\n",
    "    if 'CV' in val: ncalls = 2\n",
    "    try: df = pd.read_json(data,convert_dates = False)\n",
    "    except: return html.Div()\n",
    "    Maj = pipemaker2(df, pipe, target)\n",
    "    try: \n",
    "        opt_results = Maj.fast_optimize_classifier(n_calls= int(ncalls))\n",
    "        new_pipe2 = [html.Iframe(srcDoc = estimator_html_repr(Maj.optimized_pipe[0]),height='360', width='920', hidden = 'hidden')]\n",
    "    except: \n",
    "        try: \n",
    "            opt_results = Maj.fast_optimize_classifier(n_calls= int(ncalls), is_classifier= False)\n",
    "            new_pipe2 = [html.Iframe(srcDoc = estimator_html_repr(Maj.optimized_pipe[0]), height='360', width='920', hidden = 'hidden')]\n",
    "        except:\n",
    "            new_pipe2 = [html.Iframe(srcDoc = estimator_html_repr(Maj.Pipe()), height='360', width='920', hidden = 'hidden')]\n",
    "            Maj = pipemaker2(pd.read_json(data,convert_dates = False), inpt_children_to_pipe(c_list,f_list, val), target)\n",
    "    try: \n",
    "        scores, fig  = Maj.Evaluate_model()\n",
    "        rev_table = pd.DataFrame(scores).T.reset_index().round(3)\n",
    "        graph_part = mplfig2html(fig)\n",
    "        #scoreshtml = [dash_table.DataTable( data=rev_table.to_dict('records'), columns=[{'name': str(i), 'id': str(i)} for i in rev_table.columns]),graph_part] #, style_table={'overflowX': 'auto' #style_cell={'minWidth': '180px', 'width': '180px', 'maxWidth': '180px','overflow': 'hidden','textOverflow': 'ellipsis'}\n",
    "        scoreshtml = [dcc.Graph(figure=ff.create_table(rev_table)),graph_part]\n",
    "        \n",
    "    except: scoreshtml =  [html.H4('Failed evaluate scores: is it a regressor?', className=\"display-3\") ,html.H4('Failed evaluate scores: is it a regressor?', className=\"display-3\") ]\n",
    "        \n",
    "    ##### shapley graphs\n",
    "    if Maj.optimized_pipe[1] == 0: clf = Maj.Pipe()\n",
    "    else: clf = Maj.optimized_pipe[0]\n",
    "    \n",
    "    new_pipe = html.Iframe(srcDoc = estimator_html_repr(clf), height='360', width='920', hidden = True)\n",
    "    #fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    shap.initjs()\n",
    "    dat_trans = Maj.named_preprocessor()\n",
    "    try: \n",
    "        explainer = shap.TreeExplainer(clf['classifier'].fit(dat_trans, Maj.df[Maj.TG]), dat_trans) ######## added dat_trans here ____________________remove if breaks!!!\n",
    "        shap_values = explainer.shap_values(dat_trans, check_additivity=False)        #,feature_perturbation = \"tree_path_dependent\"\n",
    "    except: \n",
    "        explainer = shap.Explainer(clf['classifier'].fit(dat_trans, Maj.df[Maj.TG]), dat_trans) \n",
    "        shap_values = explainer.shap_values(dat_trans)\n",
    "    \n",
    "    #### summary plot\n",
    "    fig_summary, ax = plt.subplots(figsize=(15, 15))\n",
    "    shap.summary_plot(shap_values,dat_trans, plot_type='bar',plot_size=(10,10), max_display=20,show= False)\n",
    "    sumhtml = [mplfig2html(fig_summary)]\n",
    "\n",
    "    #### force-plot\n",
    "    try: a = [_force_plot_html(explainer.expected_value[i], shap_values[i], dat_trans) for i in range(len(shap_values))]\n",
    "    except: a = [_force_plot_html(explainer.expected_value, shap_values, dat_trans) ]\n",
    "    #a = []\n",
    "    ### dependence matrix\n",
    "    try:\n",
    "        ivalues = shap.TreeExplainer(clf['classifier'].fit(dat_trans, Maj.df[Maj.TG])).shap_interaction_values(dat_trans)\n",
    "        figdm, axdm = plt.subplots(len( dat_trans.columns),  len(dat_trans.columns), figsize=(15, 15))\n",
    "        shap.summary_plot(ivalues, dat_trans, show= False)\n",
    "        ####erase here if necessary\n",
    "        figdm = plt.gcf()\n",
    "        figdm.set_figheight(15)\n",
    "        figdm.set_figwidth(15)\n",
    "        figdm.tight_layout()\n",
    "        fig2html = mplfig2html(figdm)\n",
    "    except:\n",
    "        fig2html = html.H6(\"Shapley interaction matrix only available for tree-based models\")\n",
    "        \n",
    "    #### heatmap\n",
    "    try: \n",
    "        try : shap.plots.heatmap(explainer(dat_trans), show= False) \n",
    "        except : shap.plots.heatmap(explainer(dat_trans), show= False, check_additivity=False)\n",
    "        fig1 = plt.gcf()\n",
    "        fig1.set_figheight(15)\n",
    "        fig1.set_figwidth(15)\n",
    "        fig1.tight_layout()\n",
    "        fig1html = mplfig2html(fig1)\n",
    "        heatmapfigs = [fig1html]\n",
    "    except: \n",
    "        heatmapfigs = [html.H6('heatmap is only available in binary classification')]\n",
    "        \n",
    "    if val == \"LGBMClassifier\" or val == 'LGBMRegressor':\n",
    "        decision_tree, ax = plt.subplots(1,1, figsize=(15, 15))\n",
    "        lgbmfig = []\n",
    "        #plot_tree(clf['classifier'], ax=ax, show_info = ['leaf_count','data_percentage','internal_value', 'internal_weight', 'split_gain'])\n",
    "        #lgbmfig = [mplfig2html(decision_tree)]\n",
    "    else:\n",
    "        lgbmfig = []\n",
    "        \n",
    "    figure_names =  (['scores','roc-auc & cm'] if is_classifier(globals()[val]()) else ['score'] )+ ['feature importance'] + ['force-plot feat'+ str(i) for i in range(len(a))] + ['heatmap', 'feature interaction'] + ['decision_tree' for x in lgbmfig]\n",
    "    ml_all_figures = (scoreshtml if is_classifier(globals()[val]()) else scoreshtml[:1] ) + sumhtml +a +heatmapfigs + [fig2html] + lgbmfig\n",
    "    ml_result_tabs = dcc.Tabs([dcc.Tab(children = html.Div(content, style = {'justify-content': 'center', 'margin': '0 auto', 'width': '1760px', 'height' : '1100px'}), label = name)\n",
    "                               for name,content in zip(figure_names, ml_all_figures)], \n",
    "                              style = {'justify-content': 'center', 'margin': '0 auto', 'width': '100%'})\n",
    "\n",
    "    return [ml_result_tabs]+ new_pipe2\n",
    "    #return dbc.Container(scoreshtml+ sumhtml +a +heatmapfigs + [fig2html]+ new_pipe2)#+ new_pipe2 #fig1html,fig2html\n",
    "    \n",
    "@app.callback(Output(component_id= 'show_pipeline', component_property ='children'),\n",
    "              Input(component_id= 'preprocessing_functions', component_property ='children'),\n",
    "              Input(component_id= 'preprocessing_columns', component_property ='children'),\n",
    "              Input(component_id = 'clf_disp', component_property = 'value'),\n",
    "              Input(component_id= 'ml_results', component_property ='children'),\n",
    "              Input(component_id = 'submit_pipe', component_property = 'n_clicks') )\n",
    "def html_pipe(f_list, c_list, val, ml_children, clicked):\n",
    "    ctx = dash.callback_context.triggered[0]['prop_id'].split('.')[0]\n",
    "    if ctx != 'ml_results': \n",
    "        pipe = inpt_children_to_pipe(c_list,f_list, val)\n",
    "        return html.Iframe(srcDoc = estimator_html_repr(pipe),  height='360', width='920',style = {'border-style':'none', 'frameborder':'none'})    \n",
    "    else: \n",
    "        try: ret = ml_children[-1]['props']['srcDoc']\n",
    "        except: return html.Iframe(srcDoc = estimator_html_repr(inpt_children_to_pipe(c_list,f_list, val)), height='360', width='920',  style = {'border-style':'none', 'frameborder':'none'})  \n",
    "    return html.Iframe(srcDoc = ret,  height='360', width='920', style = {'border-style':'none', 'frameborder':'none'}) #1150\n",
    "\n",
    "    \n",
    "    \n",
    "@app.callback(Output(component_id= 'preprocessing_functions', component_property ='children'),\n",
    "              Output(component_id= 'preprocessing_columns', component_property ='children'),\n",
    "              Input('n_tabs', 'value'),\n",
    "              Input(component_id = 'df', component_property = 'data'),\n",
    "              State(component_id= 'preprocessing_functions', component_property ='children'),\n",
    "              State(component_id= 'preprocessing_columns', component_property ='children') )\n",
    "def reajust_number_of_column_transformers_ML(val,data,oldf, oldc ):\n",
    "    if int(val) > len(oldf) - 1 :\n",
    "        new_func = oldf + [ dcc.Dropdown(options= transformer_options,value = ['passthrough'] ,multi=True,clearable=True, id = 'ColumnTransformer_'+ str(i)) for i in range(len(oldf)-1, int(val))]\n",
    "    elif int(val) < len(oldf) - 1:\n",
    "        new_func =  oldf[:int(val)+1]\n",
    "    else:\n",
    "        new_func =  oldf\n",
    "        \n",
    "    try: df = pd.read_json(data,convert_dates = False)\n",
    "    except: df = pd.DataFrame([0], columns = ['0'])\n",
    "    col_cat =  [x for x in df.columns if str(df[x].dtype) == 'int64']\n",
    "    col_num = [x for x in df.columns if str(df[x].dtype) == 'float64']\n",
    "    sorted_vals = [{'label': x, 'value': x} for x in col_num + col_cat] + [ {'label': x, 'value': x} for x in  df.columns if x not in ['Unnamed: 0']+ col_num + col_cat ]\n",
    "    new_c = [oldc[0]]+[ dcc.Dropdown(options= sorted_vals, value = '0' , multi=True,clearable=True, id = 'ColumnSelector_'+ str(i)) for i in range(int(val))]\n",
    "    return new_func, new_c\n",
    "\n",
    "    \n",
    "@app.callback(Output(component_id= 'UMAP_cat', component_property ='options'),  Output(component_id= 'UMAP_cat', component_property ='value'), \n",
    "              Output(component_id= 'UMAP_y', component_property ='options'),  Output(component_id= 'UMAP_y', component_property ='value'), \n",
    "              Output(component_id= 'UMAP_cont', component_property ='options'), Output(component_id= 'UMAP_cont', component_property ='value'), \n",
    "              Output(component_id= 'ML_target', component_property ='options'),  Output(component_id= 'ML_target', component_property ='value'), \n",
    "              Output(component_id= 'prophet_y', component_property ='options'),  Output(component_id= 'prophet_y', component_property ='value'),\n",
    "              Output(component_id= 'prophet_ds', component_property ='options'), # Output(component_id= 'prophet_ds', component_property ='value'),\n",
    "              Output(component_id= 'prophet_regressors', component_property ='options'),  Output(component_id= 'prophet_regressors', component_property ='value'),\n",
    "              Input(component_id= 'Merged_df', component_property ='children'), Input(component_id= 'df', component_property ='data'))\n",
    "def update_UMAP_and_ML_select_columns(inpt, data): #, columns_list_id\n",
    "    #if data != {'namespace': 'dash_html_components', 'props': {'children': None}, 'type': 'Div'} and data != None and inpt['type'] != 'Div':\n",
    "    try:\n",
    "        df = pd.read_json(data)\n",
    "        vals = [ {'label': x, 'value': x} for x in  df.columns if x not in ['Unnamed: 0']]\n",
    "        col_cat =  [x for x in df.columns if str(df[x].dtype) == 'int64']\n",
    "        col_num = [x for x in df.columns if str(df[x].dtype) == 'float64']\n",
    "        col_object = [x for x in df.columns if (str(df[x].dtype) in ['object', 'datetime64[ns]'] )]\n",
    "        sorted_vals = [{'label': x, 'value': x} for x in col_num + col_cat] + [ {'label': x, 'value': x} for x in  df.columns if x not in ['Unnamed: 0']+ col_num + col_cat ]\n",
    "        if len(col_object) > 0: \n",
    "            if 'date' in col_object: col_object = 'date'\n",
    "            elif 'datetime' in col_object: col_object = 'datetime'\n",
    "            else: col_object = col_object[0] \n",
    "        vals_object = [ {'label': x, 'value': x} for x in  df.columns  if (str(df[x].dtype) in ['object', 'datetime64[ns]'] )] \n",
    "        vals_plus_umap = sorted_vals +  [{'label': 'UMAP_'+str(x), 'value': 'UMAP_'+str(x)} for x in range(1,3)]\n",
    "        #prep_cols =  [columns_list_id[0]]+[ dcc.Dropdown(options= [{'label': x, 'value': x} for x in df.columns], value = df.columns[0] , multi=True,clearable=True, id = 'ColumnSelector_'+ str(i)) for i in range(len(columns_list_id)+1)]\n",
    "        \n",
    "        return sorted_vals, col_object,  sorted_vals, [], sorted_vals,col_num +col_cat,  sorted_vals, ['eDNA frq'], vals_plus_umap, [], vals_object,  vals_plus_umap, [] #col_object\n",
    "    \n",
    "    except:\n",
    "        return [], [], [], [], [],[], [], [], [], [],[], [],[] #, columns_list_id  str(fixed_dataset.date.dtype) == 'object'\n",
    "\n",
    "\n",
    "@app.callback(Output(component_id= 'UMAP_view', component_property ='figure'), \n",
    "              Output(component_id= 'df_with_umap', component_property ='data'),\n",
    "              Output(component_id= 'graph', component_property ='children'), \n",
    "              Output(component_id= 'cytoscape', component_property ='children'),\n",
    "              Input('UMAP_start', 'n_clicks'),\n",
    "              State('UMAP_cat', 'value'),\n",
    "              State('UMAP_cont', 'value'),\n",
    "              State('UMAP_y', 'value'),\n",
    "              State('n_neighboors', 'value'),\n",
    "              State('UMAP_radio', 'value'),\n",
    "              State(component_id= 'preprocessing_columns', component_property ='children'),\n",
    "              State(component_id= 'preprocessing_functions', component_property ='children'),\n",
    "              State(component_id = 'clf_disp', component_property = 'value'),\n",
    "              State(component_id= 'df', component_property ='data'))\n",
    "def generate_UMAP(clicked, cat_labels, cont_labels,y ,n_nb, radio_val,MLcolumns, MLfuncs, MLclassif, dataframe_json):\n",
    "    umap_list = []\n",
    "    if dataframe_json != None:\n",
    "        df = pd.read_json(dataframe_json).dropna(subset = cont_labels+cat_labels)\n",
    "        if y == None or y == []:\n",
    "            if len(cont_labels) > 0:  \n",
    "                if radio_val == 2: preprocessed_data = StandardScaler().fit_transform(df[cont_labels])\n",
    "                if radio_val == 1: preprocessed_data = df[cont_labels]\n",
    "                if radio_val == 3: preprocessed_data = inpt_children_to_pipe(MLcolumns, MLfuncs, MLclassif)['preprocessing'].fit_transform(df)\n",
    "                umap_list += [umap.UMAP(n_neighbors = n_nb).fit(preprocessed_data)]\n",
    "            if len(cat_labels) > 0:   \n",
    "                try: umap_list += [umap.UMAP(metric=\"jaccard\", n_neighbors=150).fit(make_pipeline(OneHotEncoder()).fit_transform(df[cat_labels]))] \n",
    "                except: umap_list += [umap.UMAP(metric=\"jaccard\", n_neighbors=150).fit(make_pipeline(OrdinalEncoder(), MinMaxScaler()).fit_transform(df[cat_labels]))] \n",
    "        else:# len(y) > 0:#: \n",
    "            if len(cont_labels) > 0:  \n",
    "                if radio_val == 2: preprocessed_data = StandardScaler().fit_transform(df[cont_labels])\n",
    "                if radio_val == 1: preprocessed_data = df[cont_labels]\n",
    "                if radio_val == 3: preprocessed_data = inpt_children_to_pipe(MLcolumns, MLfuncs, MLclassif)['preprocessing'].fit_transform(df)\n",
    "                umap_list +=[umap.UMAP(n_neighbors = n_nb).fit(preprocessed_data,y=df[y])]   \n",
    "            if len(cat_labels) > 0:   \n",
    "                try: umap_list += [umap.UMAP(metric=\"jaccard\", n_neighbors=150).fit(make_pipeline(OneHotEncoder()).fit_transform(df[cat_labels]),y=df[y])]\n",
    "                except: umap_list += [umap.UMAP(metric=\"jaccard\", n_neighbors=150).fit(make_pipeline(OrdinalEncoder(), MinMaxScaler()).fit_transform(df[cat_labels]),y=df[y])]\n",
    "\n",
    "        if len(umap_list) > 1: UMAP = umap_list[0] + umap_list[1]\n",
    "        elif len(umap_list) == 1: UMAP = umap_list[0] \n",
    "        else: return html.Div(), pd.DataFrame(np.zeros([1,1])).to_json() , html.Div() \n",
    "        umap_df = pd.DataFrame(UMAP.embedding_, index = df.index, columns = ['UMAP_1', 'UMAP_2'])\n",
    "        df = pd.concat([df, umap_df], axis = 1)\n",
    "        cluster = hdbscan.HDBSCAN(min_cluster_size=10, gen_min_span_tree=True)\n",
    "        df['hdbscan'] =  cluster.fit_predict(df[['UMAP_1', 'UMAP_2']])\n",
    "        df.columns = [x.replace(' ', '_') for x in df.columns]\n",
    "        dfscatter = df.copy()\n",
    "        dfscatter['hdbscan'] = dfscatter['hdbscan'].apply(str) #------- covert to str ------------\n",
    "        dfscatter = dfscatter.reset_index()\n",
    "        \n",
    "        #------------------------------------------------- generate graph of distances! ----------------------\n",
    "        default_stylesheet_cyto = [\n",
    "        {'selector': '[degree < 15]','style': {'background-color': '#223c4f','label': 'data(id)','width': \"30%\",'height': \"30%\" }},\n",
    "        {'selector': 'edge','style': {'line-color': '#223c4f', \"mid-target-arrow-color\": \"red\", \"mid-target-arrow-shape\": \"vee\" }},\n",
    "        {'selector': '[degree >= 15]', 'style': {'background-color': 'red','label': 'data(id)', 'width': \"40%\", 'height': \"40%\"}}   ]\n",
    "        if df.shape[0] < 200:\n",
    "        #cyt = nx.cytoscape_data(cluster.minimum_spanning_tree_.to_networkx())['elements']\n",
    "            cyt = nx.from_scipy_sparse_matrix(kneighbors_graph(umap_df, 2, mode = 'distance', include_self= False, n_jobs = -1), create_using=nx.DiGraph)\n",
    "\n",
    "            cytodisplay2 = cyto.Cytoscape(id='cytoscape', layout={'name': 'cose'},style={'width': '1000px', 'height': '90%'},\n",
    "                                          stylesheet = default_stylesheet_cyto,\n",
    "                                          elements = plotly_cyt3(cyt)) #{'width': '2000px', 'height': '1000px'}\n",
    "        else: \n",
    "            df_colors_sns = pd.DataFrame(MinMaxScaler(feature_range = (-2,2)).fit_transform(dfscatter[['UMAP_1','UMAP_2']]), columns = ['UMAP_1','UMAP_2'])\n",
    "            colors_sns = pd.concat([make_colormap_clustering('hdbscan', 'tab10',0, dfscatter), \n",
    "                                    make_colormap_clustering('UMAP_1', 'PiYG',1, df_colors_sns).apply(lambda x: x[:-1]), \n",
    "                                    make_colormap_clustering('UMAP_2', 'PiYG',1, df_colors_sns)], axis = 1)\n",
    "            sns.clustermap(dfscatter[[x.replace(' ', '_') for x in cont_labels]], figsize=(15,14),cmap = sns.diverging_palette(20, 220, as_cmap=True), z_score = 1, cbar_pos = None, vmax = 2, vmin = -2,\n",
    "                           row_colors =colors_sns , dendrogram_ratio=(.2, .1)) #col_cluster=False\n",
    "            fig1 = plt.gcf()\n",
    "            fig1.tight_layout()\n",
    "            cytodisplay2 = mplfig2html(fig1) #mplfig2html(fig1) --------------- edited here---------------------------\n",
    "        \n",
    "        \n",
    "        #### image from hdbscan\n",
    "        pic_IObytes = io.BytesIO()\n",
    "        fig = plt.figure(figsize = [16,6], dpi = 100)\n",
    "        ax = fig.add_subplot(121)\n",
    "        ax = cluster.single_linkage_tree_.plot(cmap='viridis', colorbar=False)\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        ax2 = cluster.minimum_spanning_tree_.plot(edge_cmap='viridis',edge_alpha=0.6, node_size=80,   edge_linewidth=2)\n",
    "        sns.despine()\n",
    "        fig.savefig(pic_IObytes,  format='png')\n",
    "        fig.clear()\n",
    "        #lpotlyfigured2 = mpl2plotlyGraph(fig)\n",
    "        pic_IObytes.seek(0)        \n",
    "        \n",
    "        graph_part = [ html.Img(src ='data:image/png;base64,{}'.format(base64.b64encode(pic_IObytes.read()).decode()))]#cytodisplay ,cytodisplay1\n",
    "        #graph_part = [lpotlyfigured2]\n",
    "\n",
    "        return px.scatter(dfscatter, x=\"UMAP_1\", y=\"UMAP_2\", color = 'hdbscan', hover_data=dfscatter.columns, template='plotly',height=1200, width=1500), df.to_json(), graph_part,cytodisplay2\n",
    "        #return dcc.Graph(figure= px.scatter(dfscatter, x=\"UMAP_1\", y=\"UMAP_2\", color = 'hdbscan', hover_data=dfscatter.columns, template='plotly',height=1200, width=1500), id = 'umap_plot_selectable'), df.to_json(), graph_part,cytodisplay2\n",
    "    return px.scatter(x = [0], y = [0]), pd.DataFrame(np.zeros([1,1])).to_json(), html.Div(), html.Div()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "@app.callback(Output(component_id= 'selected_points_umap', component_property ='data'),\n",
    "              Output(component_id= 'umap_selected_stats', component_property ='children'),\n",
    "              Input(component_id= 'UMAP_view', component_property ='selectedData'),\n",
    "              State(component_id= 'df_with_umap', component_property ='data'))\n",
    "def store_selected_umap_points(x, json_df):    \n",
    "    if x and x['points']:\n",
    "        try: \n",
    "            region = pd.concat([pd.DataFrame(i) for i in x['points']])\n",
    "            indices = region.groupby(['curveNumber','pointIndex']).first().customdata.unique()\n",
    "            dataset = pd.read_json(json_df,convert_dates = False) ######  major edits ######\n",
    "            subset = dataset.iloc[indices, :] #.apply(lambda w: w[0])\n",
    "            outer_group = dataset.copy().drop(indices, axis = 0)\n",
    "            subset_describe = subset.describe().fillna(-999).T.reset_index() #include='all'\n",
    "            subset_describe['ttest_p-value'] = subset_describe['index'].apply(lambda x: ttest_ind(subset[x], outer_group[x], equal_var = False)[1])\n",
    "            subset_describe['ttest1samp_p-value'] = subset_describe['index'].apply(lambda x: ttest_1samp(subset[x], dataset[x].mean()).pvalue)\n",
    "            \n",
    "            subset_describe = subset_describe[['index', 'count', 'mean', 'std', 'ttest_p-value','ttest1samp_p-value', 'min', 'max', '50%', '25%', '75%']]\n",
    "        except: \n",
    "            return pd.DataFrame(np.zeros([1,1])).to_json(), html.Div(str(list(region['customdata'].apply(lambda w: int(w)).unique())))\n",
    "        return subset.to_json(), dash_table.DataTable( data=subset_describe.to_dict('records'), columns=[{'name': str(i), 'id': str(i),\n",
    "                                                                                                         'type':'numeric', 'format': Format(precision=5, scheme=Scheme.fixed)\n",
    "                                                                                                         } for i in subset_describe.columns], style_table={'overflowX': 'auto'},\n",
    "                                                      style_cell={'minWidth': '180px', 'width': '90px', 'maxWidth': '180px','overflow': 'hidden','textOverflow': 'ellipsis'}, style_as_list_view=True,\n",
    "                                                     style_data_conditional=[ {'if': {'row_index': 'odd'}, 'backgroundColor': 'rgb(248, 248, 248)'} ,\n",
    "                                                                              {'if': {'column_id': 'ttest_p-value',  'filter_query': '{ttest_p-value} <= 0.05'}, 'color': 'red','fontWeight': 'bold'},\n",
    "                                                                              {'if': {'column_id': 'ttest1samp_p-value',  'filter_query': '{ttest1samp_p-value} <= 0.05'}, 'color': 'red','fontWeight': 'bold'}],  \n",
    "                                                      style_header={'backgroundColor': 'rgb(230, 230, 230)','fontWeight': 'bold'} )\n",
    "    else: \n",
    "        return pd.DataFrame(np.zeros([1,1])).to_json(), html.Div()\n",
    "    \n",
    "    \n",
    "@app.callback(Output(component_id= 'prophet_future_dates', component_property ='start_date'),\n",
    "              Output(component_id= 'prophet_future_dates', component_property ='end_date'),\n",
    "              Output(component_id= 'prophet_remove_months', component_property ='value'),\n",
    "              Output(component_id= 'prophet_remove_days_of_the_week', component_property ='value'),\n",
    "              Output(component_id= 'prophet_remove_hours', component_property ='value'),\n",
    "              Input(component_id= 'prophet_ds', component_property ='value'),\n",
    "              Input(component_id= 'prophet_regressors', component_property ='value'),\n",
    "              State(component_id= 'df', component_property ='data'),\n",
    "              )\n",
    "def add_prophet_future(ds_column,regressors, data ):\n",
    "    try: \n",
    "        df = pd.read_json(data)\n",
    "        df[ds_column] = df[ds_column].apply(pd.to_datetime)\n",
    "    except: return datetime.datetime.now().strftime('%b %d %Y'), (datetime.datetime.now()   + datetime.timedelta(365)).strftime('%b %d %Y'), [],[],[]\n",
    "    \n",
    "    if len(regressors)> 0:\n",
    "        ranged = df[df[ds_column].dt.year == df[ds_column].dt.year.max()].copy()\n",
    "        start = (ranged[ds_column].min() + datetime.timedelta(365)).strftime('%b %d %Y')\n",
    "        end = (ranged[ds_column].max() + datetime.timedelta(365)).strftime('%b %d %Y')\n",
    "    else:\n",
    "        start = (df[ds_column].max() + datetime.timedelta(1)).strftime('%b %d %Y')\n",
    "        end = (df[ds_column].max() + datetime.timedelta(366)).strftime('%b %d %Y')\n",
    "    not_present_months = [x for x in range(1,13) if x not in df[ds_column].dt.month.unique()]\n",
    "    not_present_weekdays = [x for x in range(7) if x not in df[ds_column].dt.weekday.unique()]\n",
    "    not_present_hours = [x for x in range(24) if x not in df[ds_column].dt.hour.unique()]\n",
    "        \n",
    "    return start, end, not_present_months, not_present_weekdays, not_present_hours\n",
    "    \n",
    "@app.callback(Output(component_id= 'prophet_plots', component_property ='children'), \n",
    "              Input(component_id= 'run_prophet', component_property ='n_clicks'),\n",
    "              State(component_id= 'df', component_property ='data'),\n",
    "              State(component_id= 'df_with_umap', component_property ='data'), #  , \n",
    "              State(component_id= 'prophet_y', component_property ='value'),\n",
    "              State(component_id= 'prophet_ds', component_property ='value'),\n",
    "              State(component_id= 'prophet_regressors', component_property ='value'),\n",
    "              State(component_id= 'prophet_rolling_average', component_property ='value'),\n",
    "              State(component_id= 'prophet_growth', component_property ='value'),\n",
    "              State(component_id= 'prophet_cap', component_property ='value'),\n",
    "              State(component_id= 'prophet_floor', component_property ='value'),\n",
    "              State(component_id= 'prophet_seasonality', component_property ='value'),\n",
    "              State(component_id= 'seasonality_mode', component_property ='value'),\n",
    "              State(component_id= 'season_prior', component_property ='value'),\n",
    "              State(component_id= 'prophet_n_change_points', component_property ='value'),\n",
    "              State(component_id= 'changepoint_prior', component_property ='value'),\n",
    "              State(component_id= 'changepoint_range', component_property ='value'),\n",
    "              State(component_id= 'prophet_future_dates', component_property ='start_date'),\n",
    "              State(component_id= 'prophet_future_dates', component_property ='end_date'),\n",
    "              State(component_id= 'prophet_remove_months', component_property ='value'),\n",
    "              State(component_id= 'prophet_remove_days_of_the_week', component_property ='value'),\n",
    "              State(component_id= 'prophet_remove_hours', component_property ='value'))\n",
    "def run_fbprophet(click,data, data_umap, y_column, ds_column, regressors, rolling_avg, growth, cap, floor, seasonality, season_mode, season_scale, change_points_n, change_points_prior, change_points_range,\n",
    "                 forecast_range_start,forecast_range_end, removed_months, removed_weekdays, removed_hours):\n",
    "    if data != None:\n",
    "        df = pd.read_json(data)# parse_dates=[ds_column]\n",
    "        df2 = pd.read_json(data_umap) #parse_dates=[ds_column]\n",
    "        if df2.shape[1] > df.shape[1]:\n",
    "            df = df2\n",
    "            ds_column = ds_column.replace(' ', '_')\n",
    "            y_column = y_column.replace(' ', '_')\n",
    "            regressors = [x.replace(' ', '_') for x in regressors]\n",
    "        #if use_ml_pipe == True: preprocessed_data = inpt_children_to_pipe(MLcolumns, MLfuncs, MLclassif)['preprocessing'].fit_transform(df)\n",
    "        df[ds_column] = df[ds_column].apply(pd.to_datetime)\n",
    "        df = df.sort_values(ds_column)\n",
    "        if rolling_avg > 0.1:\n",
    "            df = df.set_index(ds_column)\n",
    "            df = df.rolling(window= str(int(rolling_avg *24))+'H').mean().reset_index()\n",
    "            #resampled_data = fixed_dataset_time.resample('20d').mean().dropna().reset_index()\n",
    "            \n",
    "        df = df[[ds_column, y_column]+ regressors].dropna() ### added dropna()\n",
    "        df.columns = ['ds', 'y']+ regressors\n",
    "        \n",
    "        season_true_kwards = {x: 25 for x in seasonality} \n",
    "        season_false_kwards = {x: False for x in ['yearly_seasonality', 'weekly_seasonality', 'daily_seasonality'] if x not in seasonality}\n",
    "        \n",
    "        if growth == 'logistic':\n",
    "            df['cap'] = cap\n",
    "            df['floor'] = floor\n",
    "            \n",
    "        df = df[~(df['ds'].dt.month.isin(removed_months))]\n",
    "        df = df[~(df['ds'].dt.weekday.isin(removed_months))]\n",
    "        df = df.query('ds.dt.hour not in @removed_hours')        \n",
    "        \n",
    "        \n",
    "        fbmodel = Prophet(growth = growth,seasonality_mode =season_mode, seasonality_prior_scale =season_scale,\n",
    "                          n_changepoints= change_points_n, changepoint_prior_scale=change_points_prior , changepoint_range = change_points_range,\n",
    "                          **season_true_kwards, **season_false_kwards) #mcmc_samples=100\n",
    "\n",
    "        for i in regressors:\n",
    "            fbmodel.add_regressor(i)\n",
    "        \n",
    "        fbmodel.fit(df)\n",
    "        \n",
    "        if len(regressors) > 0:\n",
    "            future = df[df.ds.dt.year == df.ds.dt.year.max()].copy()\n",
    "            future.ds += pd.to_timedelta(365, unit='d')\n",
    "        \n",
    "        elif 'daily_seasonality' not in seasonality:\n",
    "            future = pd.DataFrame(pd.date_range(pd.to_datetime(forecast_range_start), pd.to_datetime(forecast_range_end),freq='d'), columns = ['ds'])\n",
    "        else:\n",
    "            future = pd.DataFrame(pd.date_range(pd.to_datetime(forecast_range_start), pd.to_datetime(forecast_range_end),freq='H'), columns = ['ds'])\n",
    "        \n",
    "        future = future[~(future['ds'].dt.month.isin(removed_months))]\n",
    "        future = future[~(future['ds'].dt.weekday.isin(removed_months))]\n",
    "        future = future.query('ds.dt.hour not in @removed_hours')\n",
    "\n",
    "        if growth == 'logistic':\n",
    "            future['cap'] = cap\n",
    "            future['floor'] = floor\n",
    "            \n",
    "        forecast = fbmodel.predict(pd.concat([df, future], axis = 0).reset_index())\n",
    "        \n",
    "        returnable = [dcc.Graph(figure= plot_plotly(fbmodel,forecast, figsize = (1240, 960),  xlabel=ds_column, ylabel=y_column), id = 'prophet_forecast_plot', ),\n",
    "                      dcc.Graph(figure= plot_components_plotly(fbmodel,forecast, figsize = (1240, 340)), id = 'prophet_forecast_components')]\n",
    "        if len(regressors) > 0:\n",
    "            regressor_coefs = regressor_coefficients(fbmodel)\n",
    "            regressor_coefs\n",
    "            regressor_coefs['coef_abs'] = regressor_coefs.coef.apply(abs)\n",
    "            regressor_coefs = regressor_coefs.sort_values('coef_abs', ascending = False)\n",
    "            #sns.barplot(x = 'regressor', y = 'coef', data =regressor_coefs)\n",
    "            fig00 = px.bar(regressor_coefs, x=\"regressor\", y=\"coef\", hover_data=regressor_coefs.columns, template='plotly',height=800, width=1240)\n",
    "            returnable += [dcc.Graph(figure = fig00, id='regressor_impt')] #[mplfig2html(fig1)]\n",
    "        \n",
    "        returnable_tabs = dcc.Tabs([dcc.Tab(children = content, label = name) for name,content in zip(['timeline', 'components', 'regressor coeficients'], returnable)])\n",
    "        \n",
    "        return returnable_tabs\n",
    "    \n",
    "    return html.Div()\n",
    "\n",
    "\n",
    "@app.callback(Output('gwastaviewer', 'figure'),\n",
    "              Output('pileup_options', 'options'),\n",
    "              Output('igv_options', 'options'),\n",
    "              Input('GWAS_options', 'value'),\n",
    "              Input('View GWAS', 'n_clicks'),\n",
    "              State('pvalue_threshold','value')\n",
    ")\n",
    "def generate_gwas(filename,nclicks ,thresh):\n",
    "    if filename != []:\n",
    "        name = filename.split(\"\\\\\")[-1].split('.')[0]\n",
    "        df_gwas = pd.read_csv(filename, sep = '\\t')\n",
    "        df_gwas.columns = df_gwas.columns.str.upper()\n",
    "        if \"GENE\" not in df_gwas.columns: df_gwas['GENE'] = 'UNK'\n",
    "        df_gwas['inv_prob'] = 1/np.clip(df_gwas.P, 1e-6, 1)\n",
    "        df_gwas_subset = pd.concat([df_gwas.query('P < 1e-3'),\n",
    "                                    df_gwas.query('P > 1e-3').sample(50000, weights=\"inv_prob\")] ).sort_values(['CHR', 'BP']).reset_index().dropna()\n",
    "        options_out =[{'label': f\"POS:{x['SNP']}|P:{round(-np.log10(x['P']),2)}\", 'value': x['SNP']} for y,x in df_gwas_subset.nsmallest(100, 'P').iterrows()]\n",
    "        return dashbio.ManhattanPlot( dataframe=df_gwas_subset, title = f'GWAS Manhattan Plot {name} ',\\\n",
    "                                    genomewideline_value=thresh), \\\n",
    "               options_out,options_out\n",
    "    else:\n",
    "        return px.scatter(x = [0], y = [0]), [], []\n",
    "\n",
    "    \n",
    "@app.callback(Output('pileupGraph', 'children'),\n",
    "              Input('pileup_options', 'value'))\n",
    "def view_pileup(snp):\n",
    "    if snp != None and snp != []:\n",
    "        cnt, POS = snp.split(':')\n",
    "        fig = dashbio.Pileup(\n",
    "            id = 'pileupfigure',\n",
    "            range = {'contig': cnt, 'start': int(POS)-2000, 'stop': int(POS)+2000},\n",
    "            reference = RN6_data['rat_genome_ref'],\n",
    "            tracks = RN6_data['tracks'])\n",
    "    else: fig = dcc.Graph()#px.scatter(x = [0], y = [0])\n",
    "    return fig\n",
    "    \n",
    "\n",
    "@app.callback(Output('igvGraph', 'children'),\n",
    "              Input('igv_options', 'value'),\n",
    "             Input('GWAS_options', 'value'))\n",
    "def view_igv(snp, filename):\n",
    "    if snp != None and snp != []:\n",
    "        \n",
    "        constant_track = [{ 'name': 'founders',\n",
    "                      'type': \"variant\",'format': \"vcf\", \n",
    "                      'url': founders['vcf'],\n",
    "                      'indexURL':founders['index'],\n",
    "                      'visibilityWindow': 5000000,\n",
    "                      'autoHeight': True,\n",
    "                      }]\n",
    "        name = filename.split(\"\\\\\")[-1].split('.')[0]\n",
    "        gwaslink = [x for x in gwas_link_list if name in x]\n",
    "        if gwaslink != []:\n",
    "            gwas_track = [{\n",
    "                   'type': \"gwas\", 'format': \"gwas\", 'name': f\"GWAS {name}\",\n",
    "                   'url': gwaslink[0],'indexed': True, 'autoHeight': True, 'max' : 10\n",
    "                   #'columns': { 'chromosome': 12,  'position': 13,  'value': 28},\n",
    "                    }]\n",
    "        else: gwas_track = []\n",
    "        \n",
    "        \n",
    "        cnt, POS = snp.split(':')\n",
    "        fig = dashbio.Igv(\n",
    "            id='default-igv',\n",
    "            genome='rn6',\n",
    "            locus= snp,\n",
    "            minimumBases  = 1000,\n",
    "            tracks = gwas_track+   constant_track\n",
    "        )\n",
    "    else: fig = dcc.Graph()#px.scatter(x = [0], y = [0])\n",
    "    return fig\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "app.run_server(mode='external', dev_tools_ui=True, debug=True, \n",
    "              dev_tools_hot_reload =True, threaded=True) #port = 8091, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ed3fb39-ea09-4527-b9fa-723fe44080fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "485f77c8-8cab-4da0-ae5c-5f8e3f6f49f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('gwas/locomotor1.loco.mlma', sep ='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6142e916-23b8-42ba-8e5d-ba7b75f28322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Chr.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08819525-2c76-422b-807f-d3107995c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.concat([df.query('p < 1e-3'),df.query('p > 1e-3').sample(50000)] ).sort_values(['Chr', 'bp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eef8569f-beb8-42f4-b726-7b0ca92d93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b8e1434-1e30-49f9-a116-85aedcfdcd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -2.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiag\\anaconda3\\envs\\ednash_nov2021\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n",
      "C:\\Users\\thiag\\anaconda3\\envs\\ednash_nov2021\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n",
      "C:\\Users\\thiag\\anaconda3\\envs\\ednash_nov2021\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning:\n",
      "\n",
      "The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "\n",
      "C:\\Users\\thiag\\anaconda3\\envs\\ednash_nov2021\\lib\\site-packages\\scipy\\stats\\stats.py:3629: F_onewayBadInputSizesWarning:\n",
      "\n",
      "all input arrays have length 1.  f_oneway requires that at least one input has length greater than 1.\n",
      "\n",
      "C:\\Users\\thiag\\Documents\\GitHub\\GWAS_pipeline\\statsReport.py:42: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in log10\n",
      "\n",
      "C:\\Users\\thiag\\anaconda3\\envs\\ednash_nov2021\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning:\n",
      "\n",
      "Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "\n",
      "C:\\Users\\thiag\\anaconda3\\envs\\ednash_nov2021\\lib\\site-packages\\pandas\\core\\frame.py:3678: PerformanceWarning:\n",
      "\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "\n",
      "C:\\Users\\thiag\\anaconda3\\envs\\ednash_nov2021\\lib\\site-packages\\sklearn\\base.py:488: FutureWarning:\n",
      "\n",
      "The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame([[np.NaN, -1], [np.NaN, -2]]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d359723-8b52-40d4-b138-245ff670b29e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ednash_nov2021]",
   "language": "python",
   "name": "conda-env-ednash_nov2021-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
